{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GrammarEnrichedLM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1bT3W2CziFiT6oifnhUjjuqmbZ1IxIXc2",
      "authorship_tag": "ABX9TyOP9WwaLyEOxfIKPPsXQtwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f379b23c723f4141b5b5e6c4792f246b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bbcff46cdf8f48fd907bede64427f8e7",
              "IPY_MODEL_8c3f928b47f14cbf940a7e894ee248ae",
              "IPY_MODEL_ef96c4deb3494209ab5fc02750f5bb10"
            ],
            "layout": "IPY_MODEL_8b534cf52ba1415ba1300898f3d94090"
          }
        },
        "bbcff46cdf8f48fd907bede64427f8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40785165908b46bdb3409e4bc97e0a57",
            "placeholder": "​",
            "style": "IPY_MODEL_255d07f0523648ac902ca3ad6079db16",
            "value": "Downloading: 100%"
          }
        },
        "8c3f928b47f14cbf940a7e894ee248ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60d30705354247029bfda4f02226bbcd",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c7490f48e244b4fa7d54a802b72d828",
            "value": 570
          }
        },
        "ef96c4deb3494209ab5fc02750f5bb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bdc4252ef2942868ea3fa3644c2d828",
            "placeholder": "​",
            "style": "IPY_MODEL_9af0efba52d345f6b13991888c77abb5",
            "value": " 570/570 [00:00&lt;00:00, 6.78kB/s]"
          }
        },
        "8b534cf52ba1415ba1300898f3d94090": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40785165908b46bdb3409e4bc97e0a57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "255d07f0523648ac902ca3ad6079db16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60d30705354247029bfda4f02226bbcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c7490f48e244b4fa7d54a802b72d828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bdc4252ef2942868ea3fa3644c2d828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9af0efba52d345f6b13991888c77abb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f362d19fd934664a8bfb59372f802d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25f20ca66a394ac0aa5001d5c3e5c961",
              "IPY_MODEL_ea6262c4fcab4c929a2f02085459e1dd",
              "IPY_MODEL_7fd82d4d516b4d318fb1b90409ba7882"
            ],
            "layout": "IPY_MODEL_f9dd51fb37dc42399a0716032279ef5d"
          }
        },
        "25f20ca66a394ac0aa5001d5c3e5c961": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e82f2edad2442a29571cf9817c740ce",
            "placeholder": "​",
            "style": "IPY_MODEL_37029958772843c684026bbe803e5429",
            "value": "Downloading: 100%"
          }
        },
        "ea6262c4fcab4c929a2f02085459e1dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1ea260e8be34aca968f9a9cb64e60e3",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b8e0f943bca47e3bd6c0f03db54830e",
            "value": 440473133
          }
        },
        "7fd82d4d516b4d318fb1b90409ba7882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db595fcaeb7442f1b8339e3c5e64f2a5",
            "placeholder": "​",
            "style": "IPY_MODEL_98a392ce3ac74fb2ba09efa48da5a443",
            "value": " 420M/420M [00:23&lt;00:00, 19.9MB/s]"
          }
        },
        "f9dd51fb37dc42399a0716032279ef5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e82f2edad2442a29571cf9817c740ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37029958772843c684026bbe803e5429": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1ea260e8be34aca968f9a9cb64e60e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b8e0f943bca47e3bd6c0f03db54830e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db595fcaeb7442f1b8339e3c5e64f2a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98a392ce3ac74fb2ba09efa48da5a443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimiaameri/GrammarEnriched/blob/main/GrammarEnrichedLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZouCiBkLiAR",
        "outputId": "46ed7791-71c2-4944-94fa-30fc0ab6f49b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 12.0 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.7.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "#!pip install params_flow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Section"
      ],
      "metadata": {
        "id": "QeLCfIMJ8bwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chunks=(('FENJJ',('JJ','JJR','JJS')),('FENMD',('MD')) ,('FENLS',('LS')),('FENNN',('NN','NNP','NNS','NNP$','NNPS')),('FENRB',('RB','RBR','RBS')), \n",
        "       ('FENPP',('PRP','PRP$','PP')),('FENVB',('VB','VBD','VBG','VBN','VBP','VBZ')),('FENWP',('WP','WP$')), \n",
        "       ('FENNP',('NP')), ('FENVP',('VP')),('FENPDT',('PDT')),('FENPOS',('POS')),('FENSYM',('SYM')),\n",
        "       ('FENCC',('CC')),('FENCD',('CD')),('FENDT',('DT')),('FENEX',('EX')),('FENFW',('FW')),('FENIN',('IN')),\n",
        "        ('FENTO',('TO')),('FENUH',('UH')),('FENWDT',('WDT')), ('FENWR',('WRB','WRBS')) )"
      ],
      "metadata": {
        "id": "Oikf33oqgV9C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel,BertTokenizer #,BertPreTrainedModel,BertModel, BertConfig,TFBertEmbeddings,TFBertModel,TFBertMainLayer\n",
        "#config=BertConfig()\n",
        "# #configuration.max_position_embeddings = 128\n",
        "# #model = BertModel(configuration)\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "# #configuration.max_position_embeddings = 128\n",
        "# bert = BertModel(config)\n",
        "modifiedEmbeddings=bert.embeddings.word_embeddings.weight\n",
        "modifiedEmbeddings=modifiedEmbeddings.cpu().detach().numpy()\n",
        "modifiedPosiotions=bert.embeddings.position_embeddings.weight\n",
        "modifiedPosiotions=modifiedPosiotions.cpu().detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "f379b23c723f4141b5b5e6c4792f246b",
            "bbcff46cdf8f48fd907bede64427f8e7",
            "8c3f928b47f14cbf940a7e894ee248ae",
            "ef96c4deb3494209ab5fc02750f5bb10",
            "8b534cf52ba1415ba1300898f3d94090",
            "40785165908b46bdb3409e4bc97e0a57",
            "255d07f0523648ac902ca3ad6079db16",
            "60d30705354247029bfda4f02226bbcd",
            "3c7490f48e244b4fa7d54a802b72d828",
            "2bdc4252ef2942868ea3fa3644c2d828",
            "9af0efba52d345f6b13991888c77abb5",
            "6f362d19fd934664a8bfb59372f802d9",
            "25f20ca66a394ac0aa5001d5c3e5c961",
            "ea6262c4fcab4c929a2f02085459e1dd",
            "7fd82d4d516b4d318fb1b90409ba7882",
            "f9dd51fb37dc42399a0716032279ef5d",
            "5e82f2edad2442a29571cf9817c740ce",
            "37029958772843c684026bbe803e5429",
            "b1ea260e8be34aca968f9a9cb64e60e3",
            "0b8e0f943bca47e3bd6c0f03db54830e",
            "db595fcaeb7442f1b8339e3c5e64f2a5",
            "98a392ce3ac74fb2ba09efa48da5a443"
          ]
        },
        "id": "mqRXJTuigXl4",
        "outputId": "53087106-6f07-43c4-e567-6ebbc7b466d9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f379b23c723f4141b5b5e6c4792f246b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f362d19fd934664a8bfb59372f802d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Newtokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/Thesis/uncased_L-12_H-768_A-12', \n",
        "                                          do_lower_case=True)"
      ],
      "metadata": {
        "id": "AHVjcpssMJd-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We don't need this section, we use the Params class instead of config\n",
        "'''\n",
        "class Params():\n",
        "        model_type = \"GrammarEnrichedLM\"\n",
        "        vocab_size=30522,\n",
        "        grammar_size=23,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=6,\n",
        "        num_attention_heads=6,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "\n",
        "\n",
        "        use_token_type           = True\n",
        "        use_position_embeddings  = True\n",
        "        use_grammar_embeddings  = True\n",
        "        token_type_vocab_size    = 2\n",
        "        hidden_dropout           = 0.1\n",
        "        max_position_embeddings  = 512\n",
        "        attention_probs_dropout_prob = 0.1\n",
        "        hidden_act = 'gelu'\n",
        "        hidden_dropout_prob= 0.1\n",
        "        initializer_range =0.02\n",
        "        intermediate_size= 3072\n",
        "        layer_norm_eps= 1e-12\n",
        "        num_attention_heads= 6\n",
        "        num_hidden_layers =6\n",
        "        pad_token_id= 0\n",
        "        position_embedding_type= \"absolute\"\n",
        "        type_vocab_size = 2\n",
        "        learning_rate = 0.001\n",
        "        weight_decay = 0.0001\n",
        "        batch_size = 256\n",
        "        num_epochs = 100\n",
        "        patch_size = 6  # Size of the patches to be extract from the input images\n",
        "        projection_dim = 768\n",
        "        num_heads = 2\n",
        "        transformer_layers=2\n",
        "        transformer_units = [\n",
        "            projection_dim * 2,\n",
        "            projection_dim,\n",
        "        ]  # Size of the transformer layers\n",
        "        mlp_head_units = [2048, 1024] \n",
        "Params=Params()\n",
        "'''"
      ],
      "metadata": {
        "id": "oiD2xUejdwnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data prepration  Section"
      ],
      "metadata": {
        "id": "JFLzrUTa7COS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MakingChunkEncode(row):\n",
        "    Dict={}\n",
        "    bert_tokens = []\n",
        "    Bert_Tags = []\n",
        "    Tokens = nltk.word_tokenize(row[0])\n",
        "    Tags = pos_tag(Tokens)\n",
        "    for j,orig_token in enumerate(Tokens):\n",
        "      try:\n",
        "          bert_tokens.extend(Newtokenizer.tokenize(orig_token))\n",
        "          tags=Newtokenizer.tokenize(orig_token)\n",
        "          for tag in tags:\n",
        "              Bert_Tags.append((tag,Tags[j][1]))\n",
        "      except:\n",
        "            pass\n",
        "    Grammar_FE=[j[0] for i in Bert_Tags for j in Chunks if i[1] in j[1]]\n",
        "    Grammars_POS=' '.join(Grammar_FE)\n",
        "    Grammars_POS=Grammars_POS+Tokens[-1]\n",
        "    encoded_sent= Encode_data(row[0],seq_len,cased=0)\n",
        "    encoded_grammar=Encode_data(Grammars_POS,seq_len,cased=1)\n",
        "    Dict={'Sentence':row[0],'Grammar': Grammars_POS,'Encode_Sentence': encoded_sent,'Encode_Grammar':encoded_grammar}\n",
        "    gc.collect()\n",
        "    return(Dict)\n",
        "\n",
        "def Encode_data(data,seq_len,cased):\n",
        "\n",
        "  encoded_data = Newtokenizer(data,truncation=True,add_special_tokens=True,max_length=seq_len,\n",
        "                                        pad_to_max_length=True,return_attention_mask = True)\n",
        "  input_ids = np.array(encoded_data['input_ids'])\n",
        "  token_type_ids=np.array(encoded_data['token_type_ids'])\n",
        "  attention_masks = np.array(encoded_data['attention_mask'])\n",
        "  input_data=[input_ids,token_type_ids,attention_masks]\n",
        "  matrix_data=np.vstack([input_data[0],input_data[1],input_data[2]])\n",
        "  return(matrix_data)"
      ],
      "metadata": {
        "id": "AJeKxhaN7IVo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config Section\n",
        "\n"
      ],
      "metadata": {
        "id": "hl8Z7eCvNUi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig"
      ],
      "metadata": {
        "id": "vtFIBD08vCPx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "outputId": "6726b21e-7dea-47d0-e44c-77e08ad8ea6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4bac794d4a96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMConfig(PretrainedConfig):#Config\n",
        "\n",
        "    model_type = \"GrammarEnrichedLM\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        grammar_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=6,\n",
        "        num_attention_heads=6,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        use_token_type           = True,\n",
        "        use_position_embeddings  = True,\n",
        "        use_grammar_embeddings  = True,\n",
        "        position_embedding_type= \"absolute\",\n",
        "        learning_rate = 0.001,\n",
        "        weight_decay = 0.0001,\n",
        "        batch_size = 256,\n",
        "        num_epochs = 100,\n",
        "        mlp_head_units = 2048,\n",
        "        num_heads = 6,\n",
        "        transformer_layers=6,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__( **kwargs)\n",
        "        self.vocab_size=vocab_size\n",
        "        self.grammar_size=grammar_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.num_hidden_layers=num_hidden_layers\n",
        "        self.num_attention_heads=num_attention_heads\n",
        "        self.intermediate_size=intermediate_size\n",
        "        self.hidden_act=hidden_act\n",
        "        self.hidden_dropout_prob=hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob=attention_probs_dropout_prob\n",
        "        self.max_position_embeddings=max_position_embeddings\n",
        "        self.type_vocab_size=type_vocab_size\n",
        "        self.initializer_range=initializer_range\n",
        "        self.layer_norm_eps=layer_norm_eps\n",
        "        self.pad_token_id=pad_token_id\n",
        "        self.gradient_checkpointing=gradient_checkpointing\n",
        "        self.use_token_type           = use_token_type\n",
        "        self.use_position_embeddings  = use_position_embeddings\n",
        "        self.use_grammar_embeddings  = use_grammar_embeddings\n",
        "        self.position_embedding_type= position_embedding_type\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.mlp_head_units = mlp_head_units\n",
        "        self.num_heads = num_heads\n",
        "        self.transformer_layers=transformer_layers\n",
        "\n",
        "\n",
        "configuration = GrammarEnrichedLMConfig()"
      ],
      "metadata": {
        "id": "RXlPx8zqLoWG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "4174a499-da05-4a7d-90c1-a4af24554e9c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0dcd54333844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mGrammarEnrichedLMConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#Config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GrammarEnrichedLM\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     def __init__(\n",
            "\u001b[0;31mNameError\u001b[0m: name 'PretrainedConfig' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "configuration.model_type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BKouiDt_I2-Z",
        "outputId": "3692dec3-e8d2-4981-b95b-188239230fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GrammarEnrichedLM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions Section"
      ],
      "metadata": {
        "id": "y1gn95kRNYJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from transformers import BertConfig\n",
        "from transformers.file_utils import  MULTIPLE_CHOICE_DUMMY_INPUTS,MULTIPLE_CHOICE_DUMMY_INPUTS,add_code_sample_docstrings,add_start_docstrings\n",
        "from transformers.modeling_tf_utils import TFMultipleChoiceLoss, TFPreTrainedModel,TFQuestionAnsweringLoss, TFSequenceClassificationLoss, TFTokenClassificationLoss, get_initializer, keras_serializable, shape_list\n",
        "from transformers.tokenization_utils import BatchEncoding\n",
        "\n",
        "def cast_bool_to_primitive(x, default_value=False):\n",
        "  if x is None:\n",
        "    return default_value\n",
        "  return x"
      ],
      "metadata": {
        "id": "CU8f5bIdcmYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\" Gaussian Error Linear Unit.\n",
        "    Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\n",
        "    return x * cdf\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "        x: float Tensor to perform activation.\n",
        "    Returns:\n",
        "        `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "def swish(x):\n",
        "    return x * tf.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\n",
        "    \"gelu\": tf.keras.layers.Activation(gelu),\n",
        "    \"relu\": tf.keras.activations.relu,\n",
        "    \"swish\": tf.keras.layers.Activation(swish),\n",
        "    \"gelu_new\": tf.keras.layers.Activation(gelu_new),\n",
        "}\n"
      ],
      "metadata": {
        "id": "-Tt4sCiUc2It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Section"
      ],
      "metadata": {
        "id": "ai-Gw-pABIuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"Construct the embeddings from word, grammar, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.grammar_size=configuration.grammar_size\n",
        "        self.hidden_size = configuration.hidden_size\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.max_position_embeddings,\n",
        "            configuration.hidden_size,\n",
        "            weights=[modifiedEmbeddings], trainable=True,\n",
        "            #embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"position_embeddings\",\n",
        "        )\n",
        "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.type_vocab_size,\n",
        "            configuration.hidden_size,\n",
        "            #weights=[modifiedEmbeddings], trainable=True,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_type_embeddings\",\n",
        "        )\n",
        "\n",
        "        self.token_grammar_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.grammar_size,\n",
        "            configuration.hidden_size,\n",
        "            weights=[modifiedEmbeddings], trainable=True,\n",
        "            #embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_grammar_embeddings\",\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build shared word embedding layer \"\"\"\n",
        "        with tf.name_scope(\"word_embeddings\"):\n",
        "            # Create and initialize weights. The random normal initializer was chosen\n",
        "            # arbitrarily, and works well.\n",
        "            self.word_embeddings = self.add_weight(\n",
        "                \"weight\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                initializer=get_initializer(self.initializer_range),\n",
        "            )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\", training=False):\n",
        "        \"\"\"Get token embeddings of inputs.\n",
        "        Args:\n",
        "            inputs: list of three int64 tensors with shape [batch_size, length]: (input_ids, position_ids, token_type_ids)\n",
        "            mode: string, a valid value is one of \"embedding\" and \"linear\".\n",
        "        Returns:\n",
        "            outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n",
        "                shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n",
        "                linear tensor, float32 with shape [batch_size, length, vocab_size].\n",
        "        Raises:\n",
        "            ValueError: if mode is not valid.\n",
        "\n",
        "        Shared weights logic adapted from\n",
        "            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n",
        "        \"\"\"\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs, training=training)\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        else:\n",
        "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
        "\n",
        "    def _embedding(self, inputs, training=False):\n",
        "        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n",
        "        \n",
        "        input_ids,grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds = inputs\n",
        "        \n",
        "        if input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        else:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "        if position_ids is None:\n",
        "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "        '''\n",
        "        if we want to use BERT Tokenizer Embedding matrix for Grammars\n",
        "        '''\n",
        "        if grammar_ids is None:\n",
        "            grammar_embeds = tf.gather(self.word_embeddings, grammar_ids)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)        \n",
        "        '''\n",
        "        if we want to use keras Embedding layer with randomly initialized weights for Grammars\n",
        "        '''        \n",
        "        token_grammar_embeddings = self.token_grammar_embeddings(grammar_ids)\n",
        "        \n",
        "        embeddings = inputs_embeds +grammar_embeds+ position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "        return embeddings\n",
        "\n",
        "    def _linear(self, inputs):\n",
        "        \"\"\"Computes logits by running inputs through a linear layer.\n",
        "            Args:\n",
        "                inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n",
        "            Returns:\n",
        "                float32 tensor with shape [batch_size, length, vocab_size].\n",
        "        \"\"\"\n",
        "        batch_size = shape_list(inputs)[0]\n",
        "        length = shape_list(inputs)[1]\n",
        "\n",
        "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
        "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
        "\n",
        "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n"
      ],
      "metadata": {
        "id": "B08Sf1zKdH7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings =GrammarEnrichedLMEmbeddings(configuration,name=\"embeddings\")"
      ],
      "metadata": {
        "id": "Howa_fzufy4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Section"
      ],
      "metadata": {
        "id": "pDJk00JyBOZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if configuration.hidden_size % configuration.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (configuration.hidden_size, configuration.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = configuration.num_attention_heads\n",
        "        assert configuration.hidden_size % configuration.num_attention_heads == 0\n",
        "        self.attention_head_size = int(configuration.hidden_size / configuration.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"query\"\n",
        "        )\n",
        "        self.key = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"key\"\n",
        "        )\n",
        "        self.value = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"value\"\n",
        "        )\n",
        "        self.grammar = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"grammar\"\n",
        "        )\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        batch_size = shape_list(hidden_states)[0]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "        mixed_grammar_layer = self.grammar(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
        "        grammar_layer = self.transpose_for_scores(mixed_grammar_layer, batch_size)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = tf.matmul(\n",
        "            query_layer, key_layer, transpose_b=True\n",
        "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
        "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\n",
        "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        #context_layer = tf.matmul(attention_probs, value_layer) \n",
        "        #context_layer=tf.matmul(tensorflow.matmul(attention_probs, value_layer),grammar_layer,transpose_a=True )\n",
        "\n",
        "        # ********************************** Adding another vector for grammar in attaention head\n",
        "        context_layer=tf.matmul(attention_probs, value_layer)*grammar_layer\n",
        "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer, (batch_size, -1, self.all_head_size)\n",
        "        )  # (batch_size, seq_len_q, all_head_size)\n",
        "\n",
        "        outputs = (\n",
        "            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\n",
        "        )\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JBR-HlRrBCuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "cY1pBC9F-NDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.self_attention = GrammarEnrichedLMSelfAttention(configuration, name=\"self\")\n",
        "        self.dense_output = GrammarEnrichedLMSelfOutput(configuration, name=\"output\")\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_tensor, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        self_outputs = self.self_attention(\n",
        "            [input_tensor, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4XmKHs8WVo7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMIntermediate(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.intermediate_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = configuration.hidden_act\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "UzOfC3YzVsdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "AQpO_nUFKDFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "      hidden_unit=[hidden_units*2,hidden_units]\n",
        "      for units in hidden_unit:\n",
        "          x = tf.keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "          x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "2aifHTB0XjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# layers Section"
      ],
      "metadata": {
        "id": "mEeP3iznWd6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = GrammarEnrichedLMAttention(configuration, name=\"attention\")\n",
        "        self.intermediate = GrammarEnrichedLMIntermediate(configuration, name=\"intermediate\")\n",
        "        self.grammarEnriched_output = GrammarEnrichedLMOutput(configuration, name=\"output\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        attention_outputs = self.attention(\n",
        "            [hidden_states, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.grammarEnriched_output([intermediate_output, attention_output], training=training)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "IuzeWLvGWfbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Section"
      ],
      "metadata": {
        "id": "qc5888CuWnaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer = [GrammarEnrichedLMLayer(configuration, name=\"layer_._{}\".format(i)) for i in range(configuration.num_hidden_layers)]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states = inputs\n",
        "\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                [hidden_states, attention_mask, head_mask[i], output_attentions], training=training\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if cast_bool_to_primitive(output_attentions) is True:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if cast_bool_to_primitive(output_attentions) is True:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # outputs, (hidden states), (attentions)\n"
      ],
      "metadata": {
        "id": "DiDxYk14W1PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMPooler(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size,\n",
        "            kernel_initializer=get_initializer(configuration.initializer_range),\n",
        "            activation=\"tanh\",\n",
        "            name=\"dense\",\n",
        "        )\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        return pooled_output"
      ],
      "metadata": {
        "id": "uFVUtBewY5y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMHeadTransform(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = configuration.hidden_act\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "3q149t_LY_yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GrammarEnrichedLMPredictionHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.transform = GrammarEnrichedLMHeadTransform(configuration, name=\"transform\")\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.input_embeddings = input_embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
        "        hidden_states = hidden_states + self.bias\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "JW4206NAY_vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMLMHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.predictions = GrammarEnrichedLMPredictionHead(configuration, input_embeddings, name=\"predictions\")\n",
        "\n",
        "    def call(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores"
      ],
      "metadata": {
        "id": "-At4aB5zZTxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedNSPHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seq_relationship = tf.keras.layers.Dense(\n",
        "            2, kernel_initializer=get_initializer(configuration.initializer_range), name=\"seq_relationship\"\n",
        "        )\n",
        "\n",
        "    def call(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score"
      ],
      "metadata": {
        "id": "FBb9IuRNZiTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main layers Section"
      ],
      "metadata": {
        "id": "gxrBIwRcaKrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters= GrammarEnrichedLMConfig(configuration)"
      ],
      "metadata": {
        "id": "6LG1ww8Ws36Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMainLayer(tf.keras.layers.Layer):\n",
        "    config_class=configuration\n",
        "    configuration= GrammarEnrichedLMConfig(configuration)\n",
        "\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = configuration.num_hidden_layers\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "        self.output_attentions = configuration.output_attentions\n",
        "        self.output_hidden_states = configuration.output_hidden_states\n",
        "\n",
        "        self.embeddings = GrammarEnrichedLMEmbeddings(configuration, name=\"embeddings\")\n",
        "        self.encoder = GrammarEnrichedLMEncoder(configuration, name=\"encoder\")\n",
        "        self.pooler = GrammarEnrichedLMPooler(configuration, name=\"pooler\")\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "        self.embeddings.vocab_size = value.shape[0]\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            grammar_ids=input[1]\n",
        "            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n",
        "            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n",
        "            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n",
        "            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n",
        "            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n",
        "            grammar_embeds = inputs[7] if len(inputs) > 7 else grammar_embeds\n",
        "            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n",
        "            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n",
        "            assert len(inputs) <= 10, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            grammar_ids=inputs.get(\"grammar_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            grammar_embeds=inputs.get(\"grammar_embeds\", grammar_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            assert len(inputs) <= 10, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs\n",
        "            grammar_ids=grammar_ids\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
        "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif grammar_ids is not None and grammar_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both grammar_ids and grammar_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = tf.fill(input_shape, 1)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            head_mask = [None] * self.num_hidden_layers\n",
        "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings([input_ids,grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds], training=training)\n",
        "        encoder_outputs = self.encoder(\n",
        "            [embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states],\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
        "            1:\n",
        "        ]  # add hidden_states and attentions if they are here\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "L1ocgD0_NZld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model Section"
      ],
      "metadata": {
        "id": "rkfMDtuHP9gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert import configuration_bert\n",
        "GrammarEnrichedMainLayer(configuration_bert)"
      ],
      "metadata": {
        "id": "v78Hzqq8P7vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Section"
      ],
      "metadata": {
        "id": "AJsYGafJc7dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFPreTrainedModel"
      ],
      "metadata": {
        "id": "8ffFaNA0nRY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedPreTrainedModel(TFPreTrainedModel):\n",
        "    config_class=configuration\n",
        "    base_model_prefix = \"GrammarEnrichedLM\""
      ],
      "metadata": {
        "id": "qZHEXom-iN3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GrammarEnrich Model Section"
      ],
      "metadata": {
        "id": "trRFa3kJiVXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedModel(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ZoJVud3Zc7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyxxQXH7r5-S",
        "outputId": "5bddf98e-bc6d-40df-cfe1-d3bab762b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7fd2d25b0810>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Training Section"
      ],
      "metadata": {
        "id": "m_hUuHKxizaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForPreTraining(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(configuration, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + outputs[\n",
        "            2:\n",
        "        ]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "7v-TMPuZizqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkFdwMZtmVN2",
        "outputId": "400521b2-af8a-48e4-8257-fffdeac8c568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7fd2d3948710>"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLM Section"
      ],
      "metadata": {
        "id": "-zABROqDkotk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMaskedLM(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(config, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "\n",
        "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "EKlJ9rtjkqUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSP Section"
      ],
      "metadata": {
        "id": "uPJodsZXktxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForNextSentencePrediction(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "       \n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "6T9b2-kZkvPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Example Section"
      ],
      "metadata": {
        "id": "JUETQFoKlugl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequenceClassification Section"
      ],
      "metadata": {
        "id": "l_BEkhDvi32W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForSequenceClassification(GrammarEnrichedPreTrainedModel, TFSequenceClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        " \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[11] if len(inputs) > 11 else labels\n",
        "            if len(inputs) > 11:\n",
        "                inputs = inputs[:11]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3uZfInhni5Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultipleChoice Section"
      ],
      "metadata": {
        "id": "vaU0OLRhdgBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMultipleChoice(GrammarEnrichedPreTrainedModel, TFMultipleChoiceLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            1, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def dummy_inputs(self):\n",
        "        \"\"\" Dummy inputs to build the network.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor with dummy inputs\n",
        "        \"\"\"\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
        "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
        "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
        "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
        "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
        "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
        "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            labels = inputs.get(\"labels\", labels)\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs\n",
        "\n",
        "        if input_ids is not None:\n",
        "            num_choices = shape_list(input_ids)[1]\n",
        "            seq_length = shape_list(input_ids)[2]\n",
        "        else:\n",
        "            num_choices = shape_list(inputs_embeds)[1]\n",
        "            seq_length = shape_list(inputs_embeds)[2]\n",
        "\n",
        "        flat_input_ids = tf.reshape(input_ids, (-1, seq_length)) if input_ids is not None else None\n",
        "        flat_attention_mask = tf.reshape(attention_mask, (-1, seq_length)) if attention_mask is not None else None\n",
        "        flat_token_type_ids = tf.reshape(token_type_ids, (-1, seq_length)) if token_type_ids is not None else None\n",
        "        flat_position_ids = tf.reshape(position_ids, (-1, seq_length)) if position_ids is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            tf.reshape(inputs_embeds, (-1, seq_length, shape_list(inputs_embeds)[3]))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        flat_inputs = [\n",
        "            flat_input_ids,\n",
        "            flat_attention_mask,\n",
        "            flat_token_type_ids,\n",
        "            flat_position_ids,\n",
        "            head_mask,\n",
        "            flat_inputs_embeds,\n",
        "            output_attentions,\n",
        "            output_hidden_states,\n",
        "        ]\n",
        "\n",
        "        outputs = self.bert(flat_inputs, training=training)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, reshaped_logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTv3ae4_dgSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Classification Section"
      ],
      "metadata": {
        "id": "07q05aP4OcXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForTokenClassification(GrammarEnrichedPreTrainedModel, TFTokenClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        grammar_embeds=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "       \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output, training=training)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEzPy2OpOr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Section"
      ],
      "metadata": {
        "id": "o82nViPuOVhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForQuestionAnswering(GrammarEnrichedPreTrainedModel, TFQuestionAnsweringLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.qa_outputs = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"qa_outputs\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        training=False,\n",
        "    ):\n",
        "\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            start_positions = inputs[8] if len(inputs) > 8 else start_positions\n",
        "            end_positions = inputs[9] if len(inputs) > 9 else end_positions\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            start_positions = inputs.pop(\"start_positions\", start_positions)\n",
        "            end_positions = inputs.pop(\"end_positions\", start_positions)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            grammar_embed=grammar_embeds,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            labels = {\"start_position\": start_positions}\n",
        "            labels[\"end_position\"] = end_positions\n",
        "            loss = self.compute_loss(labels, outputs[:2])\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "-tz6UE6pObD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}