{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GrammarEnrichedLM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "pDJk00JyBOZU",
        "mEeP3iznWd6Z",
        "qc5888CuWnaA",
        "JUETQFoKlugl"
      ],
      "mount_file_id": "1bT3W2CziFiT6oifnhUjjuqmbZ1IxIXc2",
      "authorship_tag": "ABX9TyNwvlD1cQLy53Y8Zvi3e+0w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimiaameri/GrammarEnriched/blob/main/GrammarEnrichedLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZouCiBkLiAR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "#!pip install params_flow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Section"
      ],
      "metadata": {
        "id": "QeLCfIMJ8bwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import gc\n",
        "import nltk\n",
        "from nltk import pos_tag\n",
        "import pickle\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "gUbtnwXMFNZR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Chunks=(('FENJJ',('JJ','JJR','JJS')),('FENMD',('MD')) ,('FENLS',('LS')),('FENNN',('NN','NNP','NNS','NNP$','NNPS')),('FENRB',('RB','RBR','RBS')), \n",
        "       ('FENPP',('PRP','PRP$','PP')),('FENVB',('VB','VBD','VBG','VBN','VBP','VBZ')),('FENWP',('WP','WP$')), \n",
        "       ('FENNP',('NP')), ('FENVP',('VP')),('FENPDT',('PDT')),('FENPOS',('POS')),('FENSYM',('SYM')),\n",
        "       ('FENCC',('CC')),('FENCD',('CD')),('FENDT',('DT')),('FENEX',('EX')),('FENFW',('FW')),('FENIN',('IN')),\n",
        "        ('FENTO',('TO')),('FENUH',('UH')),('FENWDT',('WDT')), ('FENWR',('WRB','WRBS')) )"
      ],
      "metadata": {
        "id": "Oikf33oqgV9C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel,BertTokenizer #,BertPreTrainedModel,BertModel, BertConfig,TFBertEmbeddings,TFBertModel,TFBertMainLayer\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "modifiedEmbeddings=bert.embeddings.word_embeddings.weight\n",
        "modifiedEmbeddings=modifiedEmbeddings.cpu().detach().numpy()\n",
        "modifiedPosiotions=bert.embeddings.position_embeddings.weight\n",
        "modifiedPosiotions=modifiedPosiotions.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "mqRXJTuigXl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Newtokenizer = BertTokenizer.from_pretrained('/content/drive/MyDrive/Thesis/uncased_L-12_H-768_A-12', \n",
        "                                          do_lower_case=True)"
      ],
      "metadata": {
        "id": "AHVjcpssMJd-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data prepration  Section"
      ],
      "metadata": {
        "id": "JFLzrUTa7COS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MakingChunkEncode(configuration, row):\n",
        "    Dict={}\n",
        "    bert_tokens = []\n",
        "    Bert_Tags = []\n",
        "    Tokens = nltk.word_tokenize(row[0])\n",
        "    Tags = pos_tag(Tokens)\n",
        "    for j,orig_token in enumerate(Tokens):\n",
        "      try:\n",
        "          bert_tokens.extend(Newtokenizer.tokenize(orig_token))\n",
        "          tags=Newtokenizer.tokenize(orig_token)\n",
        "          for tag in tags:\n",
        "              Bert_Tags.append((tag,Tags[j][1]))\n",
        "      except:\n",
        "            pass\n",
        "    Grammar_FE=[j[0] for i in Bert_Tags for j in Chunks if i[1] in j[1]]\n",
        "    Grammars_POS=' '.join(Grammar_FE)\n",
        "    Grammars_POS=Grammars_POS+Tokens[-1]\n",
        "    encoded_sent= Encode_data(row[0],configuration)\n",
        "    encoded_grammar=Encode_data(Grammars_POS,configuration)\n",
        "    Dict={'Sentence':row[0],'Grammar': Grammars_POS,'Encode_Sentence': encoded_sent,'Encode_Grammar':encoded_grammar}\n",
        "    gc.collect()\n",
        "    return(Dict)\n",
        "\n",
        "def Encode_data(data,configuration):\n",
        "\n",
        "  encoded_data = Newtokenizer(data,truncation=True,add_special_tokens=True,max_length=configuration.seq_length,\n",
        "                                        pad_to_max_length=True,return_attention_mask = True,return_tensors='tf')\n",
        "  input_ids = np.array(encoded_data['input_ids'])\n",
        "  token_type_ids=np.array(encoded_data['token_type_ids'])\n",
        "  attention_masks = np.array(encoded_data['attention_mask'])\n",
        "  input_data=[input_ids,token_type_ids,attention_masks]\n",
        "  matrix_data=np.vstack([input_data[0],input_data[1],input_data[2]])\n",
        "  return(matrix_data)"
      ],
      "metadata": {
        "id": "AJeKxhaN7IVo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer Section"
      ],
      "metadata": {
        "id": "eamaXPphRLAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import unicodedata\n",
        "from typing import List, Optional\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "\n",
        "from transformers.tokenization_utils import PreTrainedTokenizer, _is_control, _is_punctuation, _is_whitespace\n",
        "from transformers.tokenization_utils_fast import PreTrainedTokenizerFast\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "VOCAB_FILES_NAMES = {\"vocab_file\": \"vocab.txt\"}\n",
        "PRETRAINED_VOCAB_FILES_MAP = {\n",
        "    \"vocab_file\": {\n",
        "        \"bert-base-uncased\": \"/content/drive/MyDrive/Thesis/uncased_L-12_H-768_A-12/vocab.txt\",\n",
        "    }\n",
        "}\n",
        "\n",
        "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
        "    \"bert-base-uncased\": 512,\n",
        "    \"bert-large-uncased\": 512,\n",
        "    \"bert-base-cased\": 512,\n",
        "    \"bert-large-cased\": 512,\n",
        "}\n",
        "\n",
        "PRETRAINED_INIT_CONFIGURATION = {\n",
        "    \"bert-base-uncased\": {\"do_lower_case\": True},\n",
        "    \"bert-large-uncased\": {\"do_lower_case\": True},\n",
        "    \"bert-base-cased\": {\"do_lower_case\": False},\n",
        "    \"bert-large-cased\": {\"do_lower_case\": False},\n",
        "}\n"
      ],
      "metadata": {
        "id": "dRfHVGPPRvWA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        tokens = reader.readlines()\n",
        "    for index, token in enumerate(tokens):\n",
        "        token = token.rstrip(\"\\n\")\n",
        "        vocab[token] = index\n",
        "    return vocab\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "XRfpSJH2STnR"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertTokenizer(PreTrainedTokenizer):\n",
        "   \n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        do_lower_case=True,\n",
        "        do_basic_tokenize=True,\n",
        "        #do_grammar_tokenize=True,\n",
        "        never_split=None,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        tokenize_chinese_chars=True,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        if not os.path.isfile(vocab_file):\n",
        "            raise ValueError(\n",
        "                \"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\n",
        "                \"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(vocab_file)\n",
        "            )\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.ids_to_tokens = collections.OrderedDict([(ids, tok) for tok, ids in self.vocab.items()])\n",
        "        self.do_basic_tokenize = do_basic_tokenize\n",
        "        if do_basic_tokenize:\n",
        "            self.basic_tokenizer = BasicTokenizer(\n",
        "                do_lower_case=do_lower_case, never_split=never_split, tokenize_chinese_chars=tokenize_chinese_chars\n",
        "            )\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab, unk_token=self.unk_token)\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        return len(self.vocab)\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return dict(self.vocab, **self.added_tokens_encoder)\n",
        "\n",
        "    def _tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        if self.do_basic_tokenize:\n",
        "            for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
        "\n",
        "                # If the token is part of the never_split set\n",
        "                if token in self.basic_tokenizer.never_split:\n",
        "                    split_tokens.append(token)\n",
        "                else:\n",
        "                    split_tokens += self.wordpiece_tokenizer.tokenize(token)\n",
        "        else:\n",
        "            split_tokens = self.wordpiece_tokenizer.tokenize(text)\n",
        "        return split_tokens\n",
        "\n",
        "    def _convert_token_to_id(self, token):\n",
        "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
        "        return self.vocab.get(token, self.vocab.get(self.unk_token))\n",
        "\n",
        "    def _convert_id_to_token(self, index):\n",
        "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
        "        return self.ids_to_tokens.get(index, self.unk_token)\n",
        "\n",
        "    def convert_tokens_to_string(self, tokens):\n",
        "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
        "        out_string = \" \".join(tokens).replace(\" ##\", \"\").strip()\n",
        "        return out_string\n",
        "\n",
        "    def build_inputs_with_special_tokens(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
        "        by concatenating and adding special tokens.\n",
        "        A BERT sequence has the following format:\n",
        "\n",
        "        - single sequence: ``[CLS] X [SEP]``\n",
        "        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of IDs to which the special tokens will be added\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
        "        \"\"\"\n",
        "        if token_ids_1 is None:\n",
        "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        sep = [self.sep_token_id]\n",
        "        return cls + token_ids_0 + sep + token_ids_1 + sep\n",
        "\n",
        "    def get_special_tokens_mask(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
        "        special tokens using the tokenizer ``prepare_for_model`` method.\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
        "                Set to True if the token list is already formatted with special tokens for the model\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
        "        \"\"\"\n",
        "\n",
        "        if already_has_special_tokens:\n",
        "            if token_ids_1 is not None:\n",
        "                raise ValueError(\n",
        "                    \"You should not supply a second sequence if the provided sequence of \"\n",
        "                    \"ids is already formated with special tokens for the model.\"\n",
        "                )\n",
        "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
        "\n",
        "        if token_ids_1 is not None:\n",
        "            return [1] + ([0] * len(token_ids_0)) + [1] + ([0] * len(token_ids_1)) + [1]\n",
        "        return [1] + ([0] * len(token_ids_0)) + [1]\n",
        "\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "        ::\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]\n",
        "\n",
        "\n",
        "    def save_vocabulary(self, vocab_path):\n",
        "        \"\"\"\n",
        "        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n",
        "\n",
        "        Args:\n",
        "            vocab_path (:obj:`str`):\n",
        "                The directory in which to save the vocabulary.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`Tuple(str)`: Paths to the files saved.\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "        if os.path.isdir(vocab_path):\n",
        "            vocab_file = os.path.join(vocab_path, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
        "        else:\n",
        "            vocab_file = vocab_path\n",
        "        with open(vocab_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "            for token, token_index in sorted(self.vocab.items(), key=lambda kv: kv[1]):\n",
        "                if index != token_index:\n",
        "                    logger.warning(\n",
        "                        \"Saving vocabulary to {}: vocabulary indices are not consecutive.\"\n",
        "                        \" Please check that the vocabulary is not corrupted!\".format(vocab_file)\n",
        "                    )\n",
        "                    index = token_index\n",
        "                writer.write(token + \"\\n\")\n",
        "                index += 1\n",
        "        return (vocab_file,)\n"
      ],
      "metadata": {
        "id": "hNrK9_zNRWGd"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True, never_split=None, tokenize_chinese_chars=True):\n",
        "\n",
        "        if never_split is None:\n",
        "            never_split = []\n",
        "        self.do_lower_case = do_lower_case\n",
        "        self.never_split = set(never_split)\n",
        "        self.tokenize_chinese_chars = tokenize_chinese_chars\n",
        "\n",
        "    def tokenize(self, text, never_split=None):\n",
        "        # union() returns a new set by concatenating the two sets.\n",
        "        never_split = self.never_split.union(set(never_split)) if never_split else self.never_split\n",
        "\n",
        "        # This was added on November 1st, 2018 for the multilingual and Chinese\n",
        "        # models. This is also applied to the English models now, but it doesn't\n",
        "        # matter since the English models were not trained on any Chinese data\n",
        "        # and generally don't have any Chinese data in them (there are Chinese\n",
        "        # characters in the vocabulary because Wikipedia does have some Chinese\n",
        "        # words in the English Wikipedia.).\n",
        "        if self.tokenize_chinese_chars:\n",
        "            text = self._tokenize_chinese_chars(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case and token not in never_split:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token, never_split))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text, never_split=None):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        if never_split is not None and text in never_split:\n",
        "            return [text]\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _tokenize_chinese_chars(self, text):\n",
        "        \"\"\"Adds whitespace around any CJK character.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if self._is_chinese_char(cp):\n",
        "                output.append(\" \")\n",
        "                output.append(char)\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _is_chinese_char(self, cp):\n",
        "        \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n",
        "        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n",
        "        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n",
        "        #\n",
        "        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n",
        "        # despite its name. The modern Korean Hangul alphabet is a different block,\n",
        "        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n",
        "        # space-separated words, so they are not treated specially and handled\n",
        "        # like the all of the other languages.\n",
        "        if (\n",
        "            (cp >= 0x4E00 and cp <= 0x9FFF)\n",
        "            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n",
        "            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n",
        "            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n",
        "            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n",
        "            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n",
        "            or (cp >= 0xF900 and cp <= 0xFAFF)\n",
        "            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n",
        "        ):  #\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xFFFD or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n"
      ],
      "metadata": {
        "id": "7IyKhly7qRzp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token, max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens"
      ],
      "metadata": {
        "id": "uZBXaTtIJA38"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertTokenizerFast(PreTrainedTokenizerFast):\n",
        "    vocab_files_names = VOCAB_FILES_NAMES\n",
        "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
        "    pretrained_init_configuration = PRETRAINED_INIT_CONFIGURATION\n",
        "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_file,\n",
        "        do_lower_case=True,\n",
        "        unk_token=\"[UNK]\",\n",
        "        sep_token=\"[SEP]\",\n",
        "        pad_token=\"[PAD]\",\n",
        "        cls_token=\"[CLS]\",\n",
        "        mask_token=\"[MASK]\",\n",
        "        clean_text=True,\n",
        "        tokenize_chinese_chars=True,\n",
        "        strip_accents=None,\n",
        "        wordpieces_prefix=\"##\",\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(\n",
        "            BertWordPieceTokenizer(\n",
        "                vocab_file=vocab_file,\n",
        "                unk_token=unk_token,\n",
        "                sep_token=sep_token,\n",
        "                cls_token=cls_token,\n",
        "                clean_text=clean_text,\n",
        "                handle_chinese_chars=tokenize_chinese_chars,\n",
        "                strip_accents=strip_accents,\n",
        "                lowercase=do_lower_case,\n",
        "                wordpieces_prefix=wordpieces_prefix,\n",
        "            ),\n",
        "            unk_token=unk_token,\n",
        "            sep_token=sep_token,\n",
        "            pad_token=pad_token,\n",
        "            cls_token=cls_token,\n",
        "            mask_token=mask_token,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
        "        output = [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
        "\n",
        "        if token_ids_1:\n",
        "            output += token_ids_1 + [self.sep_token_id]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def create_token_type_ids_from_sequences(\n",
        "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
        "    ) -> List[int]:\n",
        "        \"\"\"\n",
        "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
        "        A BERT sequence pair mask has the following format:\n",
        "\n",
        "        ::\n",
        "\n",
        "            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
        "            | first sequence    | second sequence |\n",
        "\n",
        "        if token_ids_1 is None, only returns the first portion of the mask (0's).\n",
        "\n",
        "        Args:\n",
        "            token_ids_0 (:obj:`List[int]`):\n",
        "                List of ids.\n",
        "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
        "                Optional second list of IDs for sequence pairs.\n",
        "\n",
        "        Returns:\n",
        "            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n",
        "            sequence(s).\n",
        "        \"\"\"\n",
        "        sep = [self.sep_token_id]\n",
        "        cls = [self.cls_token_id]\n",
        "        if token_ids_1 is None:\n",
        "            return len(cls + token_ids_0 + sep) * [0]\n",
        "        return len(cls + token_ids_0 + sep) * [0] + len(token_ids_1 + sep) * [1]"
      ],
      "metadata": {
        "id": "3V3Lc50cp7Re"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config Section\n",
        "\n"
      ],
      "metadata": {
        "id": "hl8Z7eCvNUi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig"
      ],
      "metadata": {
        "id": "vtFIBD08vCPx"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMConfig(PretrainedConfig):#Config\n",
        "\n",
        "    model_type = \"GrammarEnrichedLM\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        grammar_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=6,\n",
        "        num_attention_heads=6,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        use_token_type           = True,\n",
        "        use_position_embeddings  = True,\n",
        "        use_grammar_embeddings  = True,\n",
        "        position_embedding_type= \"absolute\",\n",
        "        learning_rate = 0.001,\n",
        "        weight_decay = 0.0001,\n",
        "        batch_size = 256,\n",
        "        num_epochs = 100,\n",
        "        mlp_head_units = 2048,\n",
        "        num_heads = 6,\n",
        "        transformer_layers=6,\n",
        "        seq_length=512,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__( **kwargs)\n",
        "        self.vocab_size=vocab_size\n",
        "        self.grammar_size=grammar_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.num_hidden_layers=num_hidden_layers\n",
        "        self.num_attention_heads=num_attention_heads\n",
        "        self.intermediate_size=intermediate_size\n",
        "        self.hidden_act=hidden_act\n",
        "        self.hidden_dropout_prob=hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob=attention_probs_dropout_prob\n",
        "        self.max_position_embeddings=max_position_embeddings\n",
        "        self.type_vocab_size=type_vocab_size\n",
        "        self.initializer_range=initializer_range\n",
        "        self.layer_norm_eps=layer_norm_eps\n",
        "        self.pad_token_id=pad_token_id\n",
        "        self.gradient_checkpointing=gradient_checkpointing\n",
        "        self.use_token_type           = use_token_type\n",
        "        self.use_position_embeddings  = use_position_embeddings\n",
        "        self.use_grammar_embeddings  = use_grammar_embeddings\n",
        "        self.position_embedding_type= position_embedding_type\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.mlp_head_units = mlp_head_units\n",
        "        self.num_heads = num_heads\n",
        "        self.transformer_layers=transformer_layers\n",
        "        self.seq_length=seq_length\n",
        "\n",
        "\n",
        "configuration = GrammarEnrichedLMConfig()"
      ],
      "metadata": {
        "id": "RXlPx8zqLoWG"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration.seq_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKouiDt_I2-Z",
        "outputId": "e1bacf1d-bf2a-489d-95d0-d0cf829e65f2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions Section"
      ],
      "metadata": {
        "id": "y1gn95kRNYJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from transformers import BertConfig\n",
        "from transformers.file_utils import  MULTIPLE_CHOICE_DUMMY_INPUTS,MULTIPLE_CHOICE_DUMMY_INPUTS,add_code_sample_docstrings,add_start_docstrings\n",
        "from transformers.modeling_tf_utils import TFMultipleChoiceLoss, TFPreTrainedModel,TFQuestionAnsweringLoss, TFSequenceClassificationLoss, TFTokenClassificationLoss, get_initializer, keras_serializable, shape_list\n",
        "from transformers.tokenization_utils import BatchEncoding\n",
        "\n",
        "def cast_bool_to_primitive(x, default_value=False):\n",
        "  if x is None:\n",
        "    return default_value\n",
        "  return x"
      ],
      "metadata": {
        "id": "CU8f5bIdcmYV"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\" Gaussian Error Linear Unit.\n",
        "    Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\n",
        "    return x * cdf\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "        x: float Tensor to perform activation.\n",
        "    Returns:\n",
        "        `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "def swish(x):\n",
        "    return x * tf.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\n",
        "    \"gelu\": tf.keras.layers.Activation(gelu),\n",
        "    \"relu\": tf.keras.activations.relu,\n",
        "    \"swish\": tf.keras.layers.Activation(swish),\n",
        "    \"gelu_new\": tf.keras.layers.Activation(gelu_new),\n",
        "}\n"
      ],
      "metadata": {
        "id": "-Tt4sCiUc2It"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Section"
      ],
      "metadata": {
        "id": "ai-Gw-pABIuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"Construct the embeddings from word, grammar, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.grammar_size=configuration.grammar_size\n",
        "        self.hidden_size = configuration.hidden_size\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.max_position_embeddings,\n",
        "            configuration.hidden_size,\n",
        "            #weights=[modifiedPosiotions], trainable=True,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"position_embeddings\",\n",
        "        )\n",
        "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.type_vocab_size,\n",
        "            configuration.hidden_size,\n",
        "            #weights=[modifiedEmbeddings], trainable=True,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_type_embeddings\",\n",
        "        )\n",
        "\n",
        "        self.token_grammar_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.grammar_size,\n",
        "            configuration.hidden_size,\n",
        "            weights=[modifiedEmbeddings], trainable=True,\n",
        "            #embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_grammar_embeddings\",\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build shared word embedding layer \"\"\"\n",
        "        with tf.name_scope(\"word_embeddings\"):\n",
        "            # Create and initialize weights. The random normal initializer was chosen\n",
        "            # arbitrarily, and works well.\n",
        "            self.word_embeddings = self.add_weight(\n",
        "                \"weight\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                initializer=get_initializer(self.initializer_range),\n",
        "            )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\", training=False):\n",
        "        \"\"\"Get token embeddings of inputs.\n",
        "        Args:\n",
        "            inputs: list of four int64 tensors with shape [batch_size, length]: (input_ids, grammar_ids, position_ids, token_type_ids)\n",
        "            mode: string, a valid value is one of \"embedding\" and \"linear\".\n",
        "        Returns:\n",
        "            outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n",
        "                shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n",
        "                linear tensor, float32 with shape [batch_size, length, vocab_size].\n",
        "        Raises:\n",
        "            ValueError: if mode is not valid.\n",
        "\n",
        "        Shared weights logic adapted from\n",
        "            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n",
        "        \"\"\"\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs, training=training)\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        else:\n",
        "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
        "\n",
        "    def _embedding(self, inputs, training=False):\n",
        "        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n",
        "        \n",
        "        input_ids, grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds = inputs\n",
        "        \n",
        "        if input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        else:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "        if position_ids is None:\n",
        "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "        '''\n",
        "        if we want to use BERT Tokenizer Embedding matrix for Grammars\n",
        "        \n",
        "        if grammar_ids is None:\n",
        "            grammar_embeds = tf.gather(self.word_embeddings, grammar_ids)\n",
        "        '''\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)        \n",
        "        '''\n",
        "        if we want to use keras Embedding layer with randomly initialized weights for Grammars\n",
        "        '''        \n",
        "        grammar_embeds = self.token_grammar_embeddings(grammar_ids)\n",
        "        \n",
        "        embeddings = inputs_embeds +grammar_embeds+ position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "        return embeddings\n",
        "\n",
        "    def _linear(self, inputs):\n",
        "        \"\"\"Computes logits by running inputs through a linear layer.\n",
        "            Args:\n",
        "                inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n",
        "            Returns:\n",
        "                float32 tensor with shape [batch_size, length, vocab_size].\n",
        "        \"\"\"\n",
        "        batch_size = shape_list(inputs)[0]\n",
        "        length = shape_list(inputs)[1]\n",
        "\n",
        "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
        "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
        "\n",
        "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n"
      ],
      "metadata": {
        "id": "B08Sf1zKdH7h"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings =GrammarEnrichedLMEmbeddings(configuration,name=\"embeddings\")"
      ],
      "metadata": {
        "id": "Howa_fzufy4n"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Section"
      ],
      "metadata": {
        "id": "pDJk00JyBOZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if configuration.hidden_size % configuration.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (configuration.hidden_size, configuration.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = configuration.num_attention_heads\n",
        "        assert configuration.hidden_size % configuration.num_attention_heads == 0\n",
        "        self.attention_head_size = int(configuration.hidden_size / configuration.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"query\"\n",
        "        )\n",
        "        self.key = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"key\"\n",
        "        )\n",
        "        self.value = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"value\"\n",
        "        )\n",
        "        self.grammar = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"grammar\"\n",
        "        )\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        batch_size = shape_list(hidden_states)[0]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "        mixed_grammar_layer = self.grammar(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
        "        grammar_layer = self.transpose_for_scores(mixed_grammar_layer, batch_size)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = tf.matmul(\n",
        "            query_layer, key_layer, transpose_b=True\n",
        "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
        "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\n",
        "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        #context_layer = tf.matmul(attention_probs, value_layer) \n",
        "        #context_layer=tf.matmul(tensorflow.matmul(attention_probs, value_layer),grammar_layer,transpose_a=True )\n",
        "\n",
        "        # ********************************** Adding another vector for grammar in attaention head\n",
        "        context_layer=tf.matmul(attention_probs, value_layer)*grammar_layer\n",
        "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer, (batch_size, -1, self.all_head_size)\n",
        "        )  # (batch_size, seq_len_q, all_head_size)\n",
        "\n",
        "        outputs = (\n",
        "            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\n",
        "        )\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JBR-HlRrBCuT"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "cY1pBC9F-NDz"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.self_attention = GrammarEnrichedLMSelfAttention(configuration, name=\"self\")\n",
        "        self.dense_output = GrammarEnrichedLMSelfOutput(configuration, name=\"output\")\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_tensor, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        self_outputs = self.self_attention(\n",
        "            [input_tensor, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4XmKHs8WVo7z"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMIntermediate(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.intermediate_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = configuration.hidden_act\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "UzOfC3YzVsdK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "AQpO_nUFKDFC"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "      hidden_unit=[hidden_units*2,hidden_units]\n",
        "      for units in hidden_unit:\n",
        "          x = tf.keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "          x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "2aifHTB0XjyJ"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# layers Section"
      ],
      "metadata": {
        "id": "mEeP3iznWd6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = GrammarEnrichedLMAttention(configuration, name=\"attention\")\n",
        "        self.intermediate = GrammarEnrichedLMIntermediate(configuration, name=\"intermediate\")\n",
        "        self.grammarEnriched_output = GrammarEnrichedLMOutput(configuration, name=\"output\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        attention_outputs = self.attention(\n",
        "            [hidden_states, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.grammarEnriched_output([intermediate_output, attention_output], training=training)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "IuzeWLvGWfbU"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Section"
      ],
      "metadata": {
        "id": "qc5888CuWnaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer = [GrammarEnrichedLMLayer(configuration, name=\"layer_._{}\".format(i)) for i in range(configuration.num_hidden_layers)]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states = inputs\n",
        "\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                [hidden_states, attention_mask, head_mask[i], output_attentions], training=training\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if cast_bool_to_primitive(output_attentions) is True:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if cast_bool_to_primitive(output_attentions) is True:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # outputs, (hidden states), (attentions)\n"
      ],
      "metadata": {
        "id": "DiDxYk14W1PR"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMPooler(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size,\n",
        "            kernel_initializer=get_initializer(configuration.initializer_range),\n",
        "            activation=\"tanh\",\n",
        "            name=\"dense\",\n",
        "        )\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        return pooled_output"
      ],
      "metadata": {
        "id": "uFVUtBewY5y-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMHeadTransform(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = configuration.hidden_act\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "3q149t_LY_yg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GrammarEnrichedLMPredictionHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.transform = GrammarEnrichedLMHeadTransform(configuration, name=\"transform\")\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.input_embeddings = input_embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
        "        hidden_states = hidden_states + self.bias\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "JW4206NAY_vx"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMLMHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.predictions = GrammarEnrichedLMPredictionHead(configuration, input_embeddings, name=\"predictions\")\n",
        "\n",
        "    def call(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores"
      ],
      "metadata": {
        "id": "-At4aB5zZTxX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedNSPHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seq_relationship = tf.keras.layers.Dense(\n",
        "            2, kernel_initializer=get_initializer(configuration.initializer_range), name=\"seq_relationship\"\n",
        "        )\n",
        "\n",
        "    def call(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score"
      ],
      "metadata": {
        "id": "FBb9IuRNZiTz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main layers Section"
      ],
      "metadata": {
        "id": "gxrBIwRcaKrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters= GrammarEnrichedLMConfig(configuration)"
      ],
      "metadata": {
        "id": "6LG1ww8Ws36Y"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMainLayer(tf.keras.layers.Layer):\n",
        "    config_class=configuration\n",
        "    configuration= GrammarEnrichedLMConfig(configuration)\n",
        "\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = configuration.num_hidden_layers\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "        self.output_attentions = configuration.output_attentions\n",
        "        self.output_hidden_states = configuration.output_hidden_states\n",
        "\n",
        "        self.embeddings = GrammarEnrichedLMEmbeddings(configuration, name=\"embeddings\")\n",
        "        self.encoder = GrammarEnrichedLMEncoder(configuration, name=\"encoder\")\n",
        "        self.pooler = GrammarEnrichedLMPooler(configuration, name=\"pooler\")\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "        self.embeddings.vocab_size = value.shape[0]\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        #grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            attention_mask = inputs[1] if len(inputs) > 2 else attention_mask\n",
        "            token_type_ids = inputs[2] if len(inputs) > 3 else token_type_ids\n",
        "            grammar_ids = inputs[3]\n",
        "            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n",
        "            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n",
        "            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n",
        "            grammar_embeds = inputs[7] if len(inputs) > 7 else grammar_embeds\n",
        "            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n",
        "            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            grammar_ids = inputs.get(\"grammar_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            grammar_embeds=inputs.get(\"grammar_embeds\", grammar_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs[0]\n",
        "            grammar_ids=inputs[1]\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
        "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif grammar_ids is not None and grammar_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both grammar_ids and grammar_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = tf.fill(input_shape, 1)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            head_mask = [None] * self.num_hidden_layers\n",
        "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings([input_ids,grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds], training=training)\n",
        "        encoder_outputs = self.encoder(\n",
        "            [embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states],\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
        "            1:\n",
        "        ]  # add hidden_states and attentions if they are here\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "L1ocgD0_NZld"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model Section"
      ],
      "metadata": {
        "id": "rkfMDtuHP9gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GrammarEnrichedMainLayer(configuration)"
      ],
      "metadata": {
        "id": "v78Hzqq8P7vx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56da013b-d983-4312-dab0-9f3e80b3d7c9"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7f1b366cc610>"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Section"
      ],
      "metadata": {
        "id": "AJsYGafJc7dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFPreTrainedModel"
      ],
      "metadata": {
        "id": "8ffFaNA0nRY5"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedPreTrainedModel(TFPreTrainedModel):\n",
        "    config_class=configuration\n",
        "    base_model_prefix = \"GrammarEnrichedLM\""
      ],
      "metadata": {
        "id": "qZHEXom-iN3i"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GrammarEnrich Model Section"
      ],
      "metadata": {
        "id": "trRFa3kJiVXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedModel(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ZoJVud3Zc7rA"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mainmodel=GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")"
      ],
      "metadata": {
        "id": "NyxxQXH7r5-S"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Training Section"
      ],
      "metadata": {
        "id": "m_hUuHKxizaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForPreTraining(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(configuration, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + outputs[\n",
        "            2:\n",
        "        ]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "7v-TMPuZizqX"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkFdwMZtmVN2",
        "outputId": "a8a33028-0b07-467d-e231-ce76fbd7d8f0"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7f1b365a8ad0>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLM Section"
      ],
      "metadata": {
        "id": "-zABROqDkotk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMaskedLM(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(configuration, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "\n",
        "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "EKlJ9rtjkqUQ"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSP Section"
      ],
      "metadata": {
        "id": "uPJodsZXktxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForNextSentencePrediction(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "       \n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "6T9b2-kZkvPi"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Example Section"
      ],
      "metadata": {
        "id": "JUETQFoKlugl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequenceClassification Section"
      ],
      "metadata": {
        "id": "l_BEkhDvi32W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForSequenceClassification(GrammarEnrichedPreTrainedModel, TFSequenceClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        " \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[11] if len(inputs) > 11 else labels\n",
        "            if len(inputs) > 11:\n",
        "                inputs = inputs[:11]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3uZfInhni5Gp"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultipleChoice Section"
      ],
      "metadata": {
        "id": "vaU0OLRhdgBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMultipleChoice(GrammarEnrichedPreTrainedModel, TFMultipleChoiceLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            1, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def dummy_inputs(self):\n",
        "        \"\"\" Dummy inputs to build the network.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor with dummy inputs\n",
        "        \"\"\"\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
        "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
        "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
        "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
        "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
        "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
        "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            labels = inputs.get(\"labels\", labels)\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs\n",
        "\n",
        "        if input_ids is not None:\n",
        "            num_choices = shape_list(input_ids)[1]\n",
        "            seq_length = shape_list(input_ids)[2]\n",
        "        else:\n",
        "            num_choices = shape_list(inputs_embeds)[1]\n",
        "            seq_length = shape_list(inputs_embeds)[2]\n",
        "\n",
        "        flat_input_ids = tf.reshape(input_ids, (-1, seq_length)) if input_ids is not None else None\n",
        "        flat_attention_mask = tf.reshape(attention_mask, (-1, seq_length)) if attention_mask is not None else None\n",
        "        flat_token_type_ids = tf.reshape(token_type_ids, (-1, seq_length)) if token_type_ids is not None else None\n",
        "        flat_position_ids = tf.reshape(position_ids, (-1, seq_length)) if position_ids is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            tf.reshape(inputs_embeds, (-1, seq_length, shape_list(inputs_embeds)[3]))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        flat_inputs = [\n",
        "            flat_input_ids,\n",
        "            flat_attention_mask,\n",
        "            flat_token_type_ids,\n",
        "            flat_position_ids,\n",
        "            head_mask,\n",
        "            flat_inputs_embeds,\n",
        "            output_attentions,\n",
        "            output_hidden_states,\n",
        "        ]\n",
        "\n",
        "        outputs = self.bert(flat_inputs, training=training)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, reshaped_logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTv3ae4_dgSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Classification Section"
      ],
      "metadata": {
        "id": "07q05aP4OcXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForTokenClassification(GrammarEnrichedPreTrainedModel, TFTokenClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        grammar_embeds=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "       \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output, training=training)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEzPy2OpOr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Section"
      ],
      "metadata": {
        "id": "o82nViPuOVhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForQuestionAnswering(GrammarEnrichedPreTrainedModel, TFQuestionAnsweringLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.qa_outputs = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"qa_outputs\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        training=False,\n",
        "    ):\n",
        "\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            start_positions = inputs[8] if len(inputs) > 8 else start_positions\n",
        "            end_positions = inputs[9] if len(inputs) > 9 else end_positions\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            start_positions = inputs.pop(\"start_positions\", start_positions)\n",
        "            end_positions = inputs.pop(\"end_positions\", start_positions)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            grammar_embed=grammar_embeds,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            labels = {\"start_position\": start_positions}\n",
        "            labels[\"end_position\"] = end_positions\n",
        "            loss = self.compute_loss(labels, outputs[:2])\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "-tz6UE6pObD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Section"
      ],
      "metadata": {
        "id": "qyEK3e3brDWX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLM test Section"
      ],
      "metadata": {
        "id": "CeA6koKqrlZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3jOET5XPmhO",
        "outputId": "c8d80674-ac5a-435d-a75e-235b753ea6bb"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def PreProcessData(configuration, row):\n",
        "    bert_tokens = []\n",
        "    Bert_Tags = []\n",
        "    Tokens = nltk.word_tokenize(row[0])\n",
        "    Tags = pos_tag(Tokens)\n",
        "    for j,orig_token in enumerate(Tokens):\n",
        "      try:\n",
        "          bert_tokens.extend(Newtokenizer.tokenize(orig_token))\n",
        "          tags=Newtokenizer.tokenize(orig_token)\n",
        "          for tag in tags:\n",
        "              Bert_Tags.append((tag,Tags[j][1]))\n",
        "      except:\n",
        "            pass\n",
        "    Grammar_FE=[j[0] for i in Bert_Tags for j in Chunks if i[1] in j[1]]\n",
        "    Grammars_POS=' '.join(Grammar_FE)\n",
        "    Grammars_POS=Grammars_POS+Tokens[-1]\n",
        "    encoded_sent=Newtokenizer(row[0],\n",
        "                    add_special_tokens=True, \n",
        "                    return_attention_mask=True, \n",
        "                    pad_to_max_length=True, \n",
        "                    max_length=10, \n",
        "                    truncation=True,\n",
        "                    #padding=True,\n",
        "                    return_tensors='tf',)\n",
        "    encoded_grammar=Newtokenizer(Grammars_POS,\n",
        "                    add_special_tokens=True, \n",
        "                    return_attention_mask=True, \n",
        "                    pad_to_max_length=True, \n",
        "                    max_length=10, \n",
        "                    truncation=True,\n",
        "                    #padding=True,\n",
        "                    return_tensors='tf',)\n",
        "    encoded_sent['grammar_ids']=encoded_grammar.input_ids\n",
        "    inputs=encoded_sent\n",
        "    return(inputs)"
      ],
      "metadata": {
        "id": "_pqT18_tPWGw"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tf.constant(Newtokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True))[None, :]  # Batch size 1\n",
        "grammar_ids = tf.constant(Newtokenizer.encode(\"fennp, fennp fennp fenvp fenjj\", add_special_tokens=True))[None, :]  # Batch size 1\n"
      ],
      "metadata": {
        "id": "2T2hQygm3a5h"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs=PreProcessData(configuration,[\"Hello, my dog is cute\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSxBKho6A2oQ",
        "outputId": "2e35779b-f992-476e-d880-1dd61dbd562d"
      },
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2291: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xv-lWbXUFaT",
        "outputId": "f8c3a1ab-d995-4a1f-c11d-bc3c0037c9bc"
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
              "array([[  101,  7592,  1010,  2026,  3899,  2003, 10140,   102,     0,\n",
              "            0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]], dtype=int32)>, 'grammar_ids': <tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
              "array([[  101,    54,   605,    54,   175,    21, 26869,   102,     0,\n",
              "            0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model=GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "model(inputs)"
      ],
      "metadata": {
        "id": "TJiDE0RFvfYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Newtokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0P1cFNnuAVD",
        "outputId": "222cac67-dc0c-4890-a5a4-70b2ddd4af20"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[101, 7592, 1010, 2026, 3899, 2003, 10140, 102]"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NSP test Section"
      ],
      "metadata": {
        "id": "klhOAXsprnh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertForNextSentencePrediction\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = GrammarEnrichedLMForNextSentencePrediction.from_pretrained('GrammarEnrichedLM')\n",
        "\n",
        "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
        "next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n",
        "encoding = tokenizer(prompt, next_sentence, return_tensors='tf')\n",
        "\n",
        "logits = model(encoding['input_ids'], token_type_ids=encoding['token_type_ids'])[0]\n",
        "assert logits[0][0] < logits[0][1] # the next sentence was random"
      ],
      "metadata": {
        "id": "VbwzTnFXrqyb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}