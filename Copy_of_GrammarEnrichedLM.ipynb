{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GrammarEnrichedLM.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "ai-Gw-pABIuC",
        "pDJk00JyBOZU",
        "mEeP3iznWd6Z",
        "qc5888CuWnaA",
        "gxrBIwRcaKrK",
        "rkfMDtuHP9gr",
        "AJsYGafJc7dA",
        "trRFa3kJiVXt",
        "m_hUuHKxizaJ",
        "-zABROqDkotk",
        "uPJodsZXktxt",
        "JUETQFoKlugl",
        "l_BEkhDvi32W",
        "vaU0OLRhdgBm",
        "07q05aP4OcXN",
        "o82nViPuOVhn"
      ],
      "mount_file_id": "1bT3W2CziFiT6oifnhUjjuqmbZ1IxIXc2",
      "authorship_tag": "ABX9TyMT5qsNP2Ldw0s8LotmigIZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimiaameri/GrammarEnriched/blob/main/Copy_of_GrammarEnrichedLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZouCiBkLiAR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "#!pip install params_flow"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Section"
      ],
      "metadata": {
        "id": "QeLCfIMJ8bwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Chunks=(('FENJJ',('JJ','JJR','JJS')),('FENMD',('MD')) ,('FENLS',('LS')),('FENNN',('NN','NNP','NNS','NNP$','NNPS')),('FENRB',('RB','RBR','RBS')), \n",
        "       ('FENPP',('PRP','PRP$','PP')),('FENVB',('VB','VBD','VBG','VBN','VBP','VBZ')),('FENWP',('WP','WP$')), \n",
        "       ('FENNP',('NP')), ('FENVP',('VP')),('FENPDT',('PDT')),('FENPOS',('POS')),('FENSYM',('SYM')),\n",
        "       ('FENCC',('CC')),('FENCD',('CD')),('FENDT',('DT')),('FENEX',('EX')),('FENFW',('FW')),('FENIN',('IN')),\n",
        "        ('FENTO',('TO')),('FENUH',('UH')),('FENWDT',('WDT')), ('FENWR',('WRB','WRBS')) )"
      ],
      "metadata": {
        "id": "Oikf33oqgV9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel,BertTokenizer #,BertPreTrainedModel,BertModel, BertConfig,TFBertEmbeddings,TFBertModel,TFBertMainLayer\n",
        "#config=BertConfig()\n",
        "# #configuration.max_position_embeddings = 128\n",
        "# #model = BertModel(configuration)\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "# #configuration.max_position_embeddings = 128\n",
        "# bert = BertModel(config)\n",
        "modifiedEmbeddings=bert.embeddings.word_embeddings.weight\n",
        "modifiedEmbeddings=modifiedEmbeddings.cpu().detach().numpy()\n",
        "modifiedPosiotions=bert.embeddings.position_embeddings.weight\n",
        "modifiedPosiotions=modifiedPosiotions.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "mqRXJTuigXl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Newtokenizer = BertTokenizer.from_pretrained('./uncased_L-12_H-768_A-12', \n",
        "                                          do_lower_case=True)"
      ],
      "metadata": {
        "id": "AHVjcpssMJd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data prepration  Section"
      ],
      "metadata": {
        "id": "JFLzrUTa7COS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MakingChunkEncode(row):\n",
        "    Dict={}\n",
        "    bert_tokens = []\n",
        "    Bert_Tags = []\n",
        "    Tokens = nltk.word_tokenize(row[0])\n",
        "    Tags = pos_tag(Tokens)\n",
        "    for j,orig_token in enumerate(Tokens):\n",
        "      try:\n",
        "          bert_tokens.extend(Newtokenizer.tokenize(orig_token))\n",
        "          tags=Newtokenizer.tokenize(orig_token)\n",
        "          for tag in tags:\n",
        "              Bert_Tags.append((tag,Tags[j][1]))\n",
        "      except:\n",
        "            pass\n",
        "    Grammar_FE=[j[0] for i in Bert_Tags for j in Chunks if i[1] in j[1]]\n",
        "    Grammars_POS=' '.join(Grammar_FE)\n",
        "    Grammars_POS=Grammars_POS+Tokens[-1]\n",
        "    encoded_sent= Encode_data(row[0],seq_len,cased=0)\n",
        "    encoded_grammar=Encode_data(Grammars_POS,seq_len,cased=1)\n",
        "    Dict={'Sentence':row[0],'Grammar': Grammars_POS,'Encode_Sentence': encoded_sent,'Encode_Grammar':encoded_grammar}\n",
        "    gc.collect()\n",
        "    return(Dict)\n",
        "\n",
        "def Encode_data(data,seq_len,cased):\n",
        "\n",
        "  encoded_data = Newtokenizer(data,truncation=True,add_special_tokens=True,max_length=seq_len,\n",
        "                                        pad_to_max_length=True,return_attention_mask = True)\n",
        "  input_ids = np.array(encoded_data['input_ids'])\n",
        "  token_type_ids=np.array(encoded_data['token_type_ids'])\n",
        "  attention_masks = np.array(encoded_data['attention_mask'])\n",
        "  input_data=[input_ids,token_type_ids,attention_masks]\n",
        "  matrix_data=np.vstack([input_data[0],input_data[1],input_data[2]])\n",
        "  return(matrix_data)"
      ],
      "metadata": {
        "id": "AJeKxhaN7IVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config Section\n",
        "\n"
      ],
      "metadata": {
        "id": "hl8Z7eCvNUi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PretrainedConfig"
      ],
      "metadata": {
        "id": "vtFIBD08vCPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMConfig(PretrainedConfig):#Config\n",
        "\n",
        "    model_type = \"GrammarEnrichedLM\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size=30522,\n",
        "        grammar_size=30522,\n",
        "        hidden_size=768,\n",
        "        num_hidden_layers=6,\n",
        "        num_attention_heads=6,\n",
        "        intermediate_size=3072,\n",
        "        hidden_act=\"gelu\",\n",
        "        hidden_dropout_prob=0.1,\n",
        "        attention_probs_dropout_prob=0.1,\n",
        "        max_position_embeddings=512,\n",
        "        type_vocab_size=2,\n",
        "        initializer_range=0.02,\n",
        "        layer_norm_eps=1e-12,\n",
        "        pad_token_id=0,\n",
        "        gradient_checkpointing=False,\n",
        "        use_token_type           = True,\n",
        "        use_position_embeddings  = True,\n",
        "        use_grammar_embeddings  = True,\n",
        "        position_embedding_type= \"absolute\",\n",
        "        learning_rate = 0.001,\n",
        "        weight_decay = 0.0001,\n",
        "        batch_size = 256,\n",
        "        num_epochs = 100,\n",
        "        mlp_head_units = 2048,\n",
        "        num_heads = 6,\n",
        "        transformer_layers=6,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__( **kwargs)\n",
        "        self.vocab_size=vocab_size\n",
        "        self.grammar_size=grammar_size\n",
        "        self.hidden_size=hidden_size\n",
        "        self.num_hidden_layers=num_hidden_layers\n",
        "        self.num_attention_heads=num_attention_heads\n",
        "        self.intermediate_size=intermediate_size\n",
        "        self.hidden_act=hidden_act\n",
        "        self.hidden_dropout_prob=hidden_dropout_prob\n",
        "        self.attention_probs_dropout_prob=attention_probs_dropout_prob\n",
        "        self.max_position_embeddings=max_position_embeddings\n",
        "        self.type_vocab_size=type_vocab_size\n",
        "        self.initializer_range=initializer_range\n",
        "        self.layer_norm_eps=layer_norm_eps\n",
        "        self.pad_token_id=pad_token_id\n",
        "        self.gradient_checkpointing=gradient_checkpointing\n",
        "        self.use_token_type           = use_token_type\n",
        "        self.use_position_embeddings  = use_position_embeddings\n",
        "        self.use_grammar_embeddings  = use_grammar_embeddings\n",
        "        self.position_embedding_type= position_embedding_type\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.mlp_head_units = mlp_head_units\n",
        "        self.num_heads = num_heads\n",
        "        self.transformer_layers=transformer_layers\n",
        "\n",
        "\n",
        "configuration = GrammarEnrichedLMConfig()"
      ],
      "metadata": {
        "id": "RXlPx8zqLoWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "configuration.model_type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "BKouiDt_I2-Z",
        "outputId": "3692dec3-e8d2-4981-b95b-188239230fb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'GrammarEnrichedLM'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions Section"
      ],
      "metadata": {
        "id": "y1gn95kRNYJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "#from transformers import BertConfig\n",
        "from transformers.file_utils import  MULTIPLE_CHOICE_DUMMY_INPUTS,MULTIPLE_CHOICE_DUMMY_INPUTS,add_code_sample_docstrings,add_start_docstrings\n",
        "from transformers.modeling_tf_utils import TFMultipleChoiceLoss, TFPreTrainedModel,TFQuestionAnsweringLoss, TFSequenceClassificationLoss, TFTokenClassificationLoss, get_initializer, keras_serializable, shape_list\n",
        "from transformers.tokenization_utils import BatchEncoding\n",
        "\n",
        "def cast_bool_to_primitive(x, default_value=False):\n",
        "  if x is None:\n",
        "    return default_value\n",
        "  return x"
      ],
      "metadata": {
        "id": "CU8f5bIdcmYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gelu(x):\n",
        "    \"\"\" Gaussian Error Linear Unit.\n",
        "    Original Implementation of the gelu activation function in Google Bert repo when initially created.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.math.erf(x / tf.math.sqrt(2.0)))\n",
        "    return x * cdf\n",
        "\n",
        "def gelu_new(x):\n",
        "    \"\"\"Gaussian Error Linear Unit.\n",
        "    This is a smoother version of the RELU.\n",
        "    Original paper: https://arxiv.org/abs/1606.08415\n",
        "    Args:\n",
        "        x: float Tensor to perform activation.\n",
        "    Returns:\n",
        "        `x` with the GELU activation applied.\n",
        "    \"\"\"\n",
        "    cdf = 0.5 * (1.0 + tf.tanh((np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "def swish(x):\n",
        "    return x * tf.sigmoid(x)\n",
        "\n",
        "ACT2FN = {\n",
        "    \"gelu\": tf.keras.layers.Activation(gelu),\n",
        "    \"relu\": tf.keras.activations.relu,\n",
        "    \"swish\": tf.keras.layers.Activation(swish),\n",
        "    \"gelu_new\": tf.keras.layers.Activation(gelu_new),\n",
        "}\n"
      ],
      "metadata": {
        "id": "-Tt4sCiUc2It"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Section"
      ],
      "metadata": {
        "id": "ai-Gw-pABIuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEmbeddings(tf.keras.layers.Layer):\n",
        "    \"\"\"Construct the embeddings from word, grammar, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.grammar_size=configuration.grammar_size\n",
        "        self.hidden_size = configuration.hidden_size\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "\n",
        "        self.position_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.max_position_embeddings,\n",
        "            configuration.hidden_size,\n",
        "            weights=[modifiedEmbeddings], trainable=True,\n",
        "            #embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"position_embeddings\",\n",
        "        )\n",
        "        self.token_type_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.type_vocab_size,\n",
        "            configuration.hidden_size,\n",
        "            #weights=[modifiedEmbeddings], trainable=True,\n",
        "            embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_type_embeddings\",\n",
        "        )\n",
        "\n",
        "        self.token_grammar_embeddings = tf.keras.layers.Embedding(\n",
        "            configuration.grammar_size,\n",
        "            configuration.hidden_size,\n",
        "            weights=[modifiedEmbeddings], trainable=True,\n",
        "            #embeddings_initializer=get_initializer(self.initializer_range),\n",
        "            name=\"token_grammar_embeddings\",\n",
        "        )\n",
        "\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build shared word embedding layer \"\"\"\n",
        "        with tf.name_scope(\"word_embeddings\"):\n",
        "            # Create and initialize weights. The random normal initializer was chosen\n",
        "            # arbitrarily, and works well.\n",
        "            self.word_embeddings = self.add_weight(\n",
        "                \"weight\",\n",
        "                shape=[self.vocab_size, self.hidden_size],\n",
        "                initializer=get_initializer(self.initializer_range),\n",
        "            )\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mode=\"embedding\", training=False):\n",
        "        \"\"\"Get token embeddings of inputs.\n",
        "        Args:\n",
        "            inputs: list of three int64 tensors with shape [batch_size, length]: (input_ids, position_ids, token_type_ids)\n",
        "            mode: string, a valid value is one of \"embedding\" and \"linear\".\n",
        "        Returns:\n",
        "            outputs: (1) If mode == \"embedding\", output embedding tensor, float32 with\n",
        "                shape [batch_size, length, embedding_size]; (2) mode == \"linear\", output\n",
        "                linear tensor, float32 with shape [batch_size, length, vocab_size].\n",
        "        Raises:\n",
        "            ValueError: if mode is not valid.\n",
        "\n",
        "        Shared weights logic adapted from\n",
        "            https://github.com/tensorflow/models/blob/a009f4fb9d2fc4949e32192a944688925ef78659/official/transformer/v2/embedding_layer.py#L24\n",
        "        \"\"\"\n",
        "        if mode == \"embedding\":\n",
        "            return self._embedding(inputs, training=training)\n",
        "        elif mode == \"linear\":\n",
        "            return self._linear(inputs)\n",
        "        else:\n",
        "            raise ValueError(\"mode {} is not valid.\".format(mode))\n",
        "\n",
        "    def _embedding(self, inputs, training=False):\n",
        "        \"\"\"Applies embedding based on inputs tensor.\"\"\"\n",
        "        \n",
        "        input_ids,grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds = inputs\n",
        "        \n",
        "        if input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        else:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "        if position_ids is None:\n",
        "            position_ids = tf.range(seq_length, dtype=tf.int32)[tf.newaxis, :]\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "        '''\n",
        "        if we want to use BERT Tokenizer Embedding matrix for Grammars\n",
        "        '''\n",
        "        if grammar_ids is None:\n",
        "            grammar_embeds = tf.gather(self.word_embeddings, grammar_ids)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = tf.gather(self.word_embeddings, input_ids)\n",
        "\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)        \n",
        "        '''\n",
        "        if we want to use keras Embedding layer with randomly initialized weights for Grammars\n",
        "        '''        \n",
        "        token_grammar_embeddings = self.token_grammar_embeddings(grammar_ids)\n",
        "        \n",
        "        embeddings = inputs_embeds +grammar_embeds+ position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings, training=training)\n",
        "        return embeddings\n",
        "\n",
        "    def _linear(self, inputs):\n",
        "        \"\"\"Computes logits by running inputs through a linear layer.\n",
        "            Args:\n",
        "                inputs: A float32 tensor with shape [batch_size, length, hidden_size]\n",
        "            Returns:\n",
        "                float32 tensor with shape [batch_size, length, vocab_size].\n",
        "        \"\"\"\n",
        "        batch_size = shape_list(inputs)[0]\n",
        "        length = shape_list(inputs)[1]\n",
        "\n",
        "        x = tf.reshape(inputs, [-1, self.hidden_size])\n",
        "        logits = tf.matmul(x, self.word_embeddings, transpose_b=True)\n",
        "\n",
        "        return tf.reshape(logits, [batch_size, length, self.vocab_size])\n"
      ],
      "metadata": {
        "id": "B08Sf1zKdH7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings =GrammarEnrichedLMEmbeddings(configuration,name=\"embeddings\")"
      ],
      "metadata": {
        "id": "Howa_fzufy4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Section"
      ],
      "metadata": {
        "id": "pDJk00JyBOZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        if configuration.hidden_size % configuration.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (configuration.hidden_size, configuration.num_attention_heads)\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = configuration.num_attention_heads\n",
        "        assert configuration.hidden_size % configuration.num_attention_heads == 0\n",
        "        self.attention_head_size = int(configuration.hidden_size / configuration.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"query\"\n",
        "        )\n",
        "        self.key = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"key\"\n",
        "        )\n",
        "        self.value = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"value\"\n",
        "        )\n",
        "        self.grammar = tf.keras.layers.Dense(\n",
        "            self.all_head_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"grammar\"\n",
        "        )\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_attention_heads, self.attention_head_size))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        batch_size = shape_list(hidden_states)[0]\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "        mixed_grammar_layer = self.grammar(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer, batch_size)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer, batch_size)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer, batch_size)\n",
        "        grammar_layer = self.transpose_for_scores(mixed_grammar_layer, batch_size)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = tf.matmul(\n",
        "            query_layer, key_layer, transpose_b=True\n",
        "        )  # (batch size, num_heads, seq_len_q, seq_len_k)\n",
        "        dk = tf.cast(shape_list(key_layer)[-1], tf.float32)  # scale attention_scores\n",
        "        attention_scores = attention_scores / tf.math.sqrt(dk)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in TFBertModel call() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = tf.nn.softmax(attention_scores, axis=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs, training=training)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        #context_layer = tf.matmul(attention_probs, value_layer) \n",
        "        #context_layer=tf.matmul(tensorflow.matmul(attention_probs, value_layer),grammar_layer,transpose_a=True )\n",
        "\n",
        "        # ********************************** Adding another vector for grammar in attaention head\n",
        "        context_layer=tf.matmul(attention_probs, value_layer)*grammar_layer\n",
        "        context_layer = tf.transpose(context_layer, perm=[0, 2, 1, 3])\n",
        "        context_layer = tf.reshape(\n",
        "            context_layer, (batch_size, -1, self.all_head_size)\n",
        "        )  # (batch_size, seq_len_q, all_head_size)\n",
        "\n",
        "        outputs = (\n",
        "            (context_layer, attention_probs) if cast_bool_to_primitive(output_attentions) is True else (context_layer,)\n",
        "        )\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "JBR-HlRrBCuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMSelfOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "cY1pBC9F-NDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.self_attention = GrammarEnrichedLMSelfAttention(configuration, name=\"self\")\n",
        "        self.dense_output = GrammarEnrichedLMSelfOutput(configuration, name=\"output\")\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        input_tensor, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        self_outputs = self.self_attention(\n",
        "            [input_tensor, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = self.dense_output([self_outputs[0], input_tensor], training=training)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "4XmKHs8WVo7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMIntermediate(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.intermediate_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = configuration.hidden_act\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "UzOfC3YzVsdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMOutput(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, input_tensor = inputs\n",
        "\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states, training=training)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "AQpO_nUFKDFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(x, hidden_units, dropout_rate):\n",
        "      hidden_unit=[hidden_units*2,hidden_units]\n",
        "      for units in hidden_unit:\n",
        "          x = tf.keras.layers.Dense(units, activation=tf.nn.gelu)(x)\n",
        "          x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "2aifHTB0XjyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# layers Section"
      ],
      "metadata": {
        "id": "mEeP3iznWd6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMLayer(tf.keras.layers.Layer):\n",
        "  \n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.attention = GrammarEnrichedLMAttention(configuration, name=\"attention\")\n",
        "        self.intermediate = GrammarEnrichedLMIntermediate(configuration, name=\"intermediate\")\n",
        "        self.grammarEnriched_output = GrammarEnrichedLMOutput(configuration, name=\"output\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions = inputs\n",
        "\n",
        "        attention_outputs = self.attention(\n",
        "            [hidden_states, attention_mask, head_mask, output_attentions], training=training\n",
        "        )\n",
        "        attention_output = attention_outputs[0]\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.grammarEnriched_output([intermediate_output, attention_output], training=training)\n",
        "        outputs = (layer_output,) + attention_outputs[1:]  # add attentions if we output them\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "IuzeWLvGWfbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Section"
      ],
      "metadata": {
        "id": "qc5888CuWnaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer = [GrammarEnrichedLMLayer(configuration, name=\"layer_._{}\".format(i)) for i in range(configuration.num_hidden_layers)]\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states = inputs\n",
        "\n",
        "        all_hidden_states = ()\n",
        "        all_attentions = ()\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_outputs = layer_module(\n",
        "                [hidden_states, attention_mask, head_mask[i], output_attentions], training=training\n",
        "            )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if cast_bool_to_primitive(output_attentions) is True:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        # Add last layer\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "        if cast_bool_to_primitive(output_hidden_states) is True:\n",
        "            outputs = outputs + (all_hidden_states,)\n",
        "        if cast_bool_to_primitive(output_attentions) is True:\n",
        "            outputs = outputs + (all_attentions,)\n",
        "        return outputs  # outputs, (hidden states), (attentions)\n"
      ],
      "metadata": {
        "id": "DiDxYk14W1PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMPooler(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size,\n",
        "            kernel_initializer=get_initializer(configuration.initializer_range),\n",
        "            activation=\"tanh\",\n",
        "            name=\"dense\",\n",
        "        )\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        return pooled_output"
      ],
      "metadata": {
        "id": "uFVUtBewY5y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMHeadTransform(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = tf.keras.layers.Dense(\n",
        "            configuration.hidden_size, kernel_initializer=get_initializer(configuration.initializer_range), name=\"dense\"\n",
        "        )\n",
        "        if isinstance(configuration.hidden_act, str):\n",
        "            self.transform_act_fn = ACT2FN[configuration.hidden_act]\n",
        "        else:\n",
        "            self.transform_act_fn = configuration.hidden_act\n",
        "        self.LayerNorm = tf.keras.layers.LayerNormalization(epsilon=configuration.layer_norm_eps, name=\"LayerNorm\")\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "metadata": {
        "id": "3q149t_LY_yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GrammarEnrichedLMPredictionHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = configuration.vocab_size\n",
        "        self.transform = GrammarEnrichedLMHeadTransform(configuration, name=\"transform\")\n",
        "\n",
        "        # The output weights are the same as the input embeddings, but there is\n",
        "        # an output-only bias for each token.\n",
        "        self.input_embeddings = input_embeddings\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.bias = self.add_weight(shape=(self.vocab_size,), initializer=\"zeros\", trainable=True, name=\"bias\")\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, hidden_states):\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "        hidden_states = self.input_embeddings(hidden_states, mode=\"linear\")\n",
        "        hidden_states = hidden_states + self.bias\n",
        "        return hidden_states"
      ],
      "metadata": {
        "id": "JW4206NAY_vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMLMHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, input_embeddings, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.predictions = GrammarEnrichedLMPredictionHead(configuration, input_embeddings, name=\"predictions\")\n",
        "\n",
        "    def call(self, sequence_output):\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "        return prediction_scores"
      ],
      "metadata": {
        "id": "-At4aB5zZTxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedNSPHead(tf.keras.layers.Layer):\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.seq_relationship = tf.keras.layers.Dense(\n",
        "            2, kernel_initializer=get_initializer(configuration.initializer_range), name=\"seq_relationship\"\n",
        "        )\n",
        "\n",
        "    def call(self, pooled_output):\n",
        "        seq_relationship_score = self.seq_relationship(pooled_output)\n",
        "        return seq_relationship_score"
      ],
      "metadata": {
        "id": "FBb9IuRNZiTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main layers Section"
      ],
      "metadata": {
        "id": "gxrBIwRcaKrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters= GrammarEnrichedLMConfig(configuration)"
      ],
      "metadata": {
        "id": "6LG1ww8Ws36Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedMainLayer(tf.keras.layers.Layer):\n",
        "    config_class=configuration\n",
        "    configuration= GrammarEnrichedLMConfig(configuration)\n",
        "\n",
        "    def __init__(self, configuration, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.num_hidden_layers = configuration.num_hidden_layers\n",
        "        self.initializer_range = configuration.initializer_range\n",
        "        self.output_attentions = configuration.output_attentions\n",
        "        self.output_hidden_states = configuration.output_hidden_states\n",
        "\n",
        "        self.embeddings = GrammarEnrichedLMEmbeddings(configuration, name=\"embeddings\")\n",
        "        self.encoder = GrammarEnrichedLMEncoder(configuration, name=\"encoder\")\n",
        "        self.pooler = GrammarEnrichedLMPooler(configuration, name=\"pooler\")\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "        self.embeddings.vocab_size = value.shape[0]\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        \"\"\" Prunes heads of the model.\n",
        "            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n",
        "            See base class PreTrainedModel\n",
        "        \"\"\"\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            grammar_ids=input[1]\n",
        "            attention_mask = inputs[2] if len(inputs) > 2 else attention_mask\n",
        "            token_type_ids = inputs[3] if len(inputs) > 3 else token_type_ids\n",
        "            position_ids = inputs[4] if len(inputs) > 4 else position_ids\n",
        "            head_mask = inputs[5] if len(inputs) > 5 else head_mask\n",
        "            inputs_embeds = inputs[6] if len(inputs) > 6 else inputs_embeds\n",
        "            grammar_embeds = inputs[7] if len(inputs) > 7 else grammar_embeds\n",
        "            output_attentions = inputs[8] if len(inputs) > 8 else output_attentions\n",
        "            output_hidden_states = inputs[9] if len(inputs) > 9 else output_hidden_states\n",
        "            assert len(inputs) <= 10, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            grammar_ids=inputs.get(\"grammar_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            grammar_embeds=inputs.get(\"grammar_embeds\", grammar_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            assert len(inputs) <= 10, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs\n",
        "            grammar_ids=grammar_ids\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.output_attentions\n",
        "        output_hidden_states = output_hidden_states if output_hidden_states is not None else self.output_hidden_states\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif grammar_ids is not None and grammar_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both grammar_ids and grammar_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = shape_list(input_ids)\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = shape_list(inputs_embeds)[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = tf.fill(input_shape, 1)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = tf.fill(input_shape, 0)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "\n",
        "        extended_attention_mask = tf.cast(extended_attention_mask, tf.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        if head_mask is not None:\n",
        "            raise NotImplementedError\n",
        "        else:\n",
        "            head_mask = [None] * self.num_hidden_layers\n",
        "            # head_mask = tf.constant([0] * self.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings([input_ids,grammar_ids, position_ids, token_type_ids, inputs_embeds,grammar_embeds], training=training)\n",
        "        encoder_outputs = self.encoder(\n",
        "            [embedding_output, extended_attention_mask, head_mask, output_attentions, output_hidden_states],\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "\n",
        "        outputs = (sequence_output, pooled_output,) + encoder_outputs[\n",
        "            1:\n",
        "        ]  # add hidden_states and attentions if they are here\n",
        "        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "L1ocgD0_NZld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model Section"
      ],
      "metadata": {
        "id": "rkfMDtuHP9gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.models.bert import configuration_bert\n",
        "GrammarEnrichedMainLayer(configuration_bert)"
      ],
      "metadata": {
        "id": "v78Hzqq8P7vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Section"
      ],
      "metadata": {
        "id": "AJsYGafJc7dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFPreTrainedModel"
      ],
      "metadata": {
        "id": "8ffFaNA0nRY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedPreTrainedModel(TFPreTrainedModel):\n",
        "    config_class=configuration\n",
        "    base_model_prefix = \"GrammarEnrichedLM\""
      ],
      "metadata": {
        "id": "qZHEXom-iN3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GrammarEnrich Model Section"
      ],
      "metadata": {
        "id": "trRFa3kJiVXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedModel(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "ZoJVud3Zc7rA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyxxQXH7r5-S",
        "outputId": "5bddf98e-bc6d-40df-cfe1-d3bab762b27e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7fd2d25b0810>"
            ]
          },
          "metadata": {},
          "execution_count": 200
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Training Section"
      ],
      "metadata": {
        "id": "m_hUuHKxizaJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForPreTraining(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(configuration, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output, pooled_output = outputs[:2]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (prediction_scores, seq_relationship_score,) + outputs[\n",
        "            2:\n",
        "        ]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "7v-TMPuZizqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkFdwMZtmVN2",
        "outputId": "400521b2-af8a-48e4-8257-fffdeac8c568"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.GrammarEnrichedMainLayer at 0x7fd2d3948710>"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLM Section"
      ],
      "metadata": {
        "id": "-zABROqDkotk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMaskedLM(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.mlm = GrammarEnrichedMLMHead(config, self.GrammarEnrichedLM.embeddings, name=\"mlm___cls\")\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.GrammarEnrichedLM.embeddings\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "        prediction_scores = self.mlm(sequence_output, training=kwargs.get(\"training\", False))\n",
        "\n",
        "        outputs = (prediction_scores,) + outputs[2:]  # Add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # prediction_scores, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "EKlJ9rtjkqUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NSP Section"
      ],
      "metadata": {
        "id": "uPJodsZXktxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForNextSentencePrediction(GrammarEnrichedPreTrainedModel):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.nsp = GrammarEnrichedNSPHead(configuration, name=\"nsp___cls\")\n",
        "\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "       \n",
        "        outputs = self.GrammarEnrichedLM(inputs, **kwargs)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "        seq_relationship_score = self.nsp(pooled_output)\n",
        "\n",
        "        outputs = (seq_relationship_score,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        return outputs  # seq_relationship_score, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "6T9b2-kZkvPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tune Example Section"
      ],
      "metadata": {
        "id": "JUETQFoKlugl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SequenceClassification Section"
      ],
      "metadata": {
        "id": "l_BEkhDvi32W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForSequenceClassification(GrammarEnrichedPreTrainedModel, TFSequenceClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        " \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[11] if len(inputs) > 11 else labels\n",
        "            if len(inputs) > 11:\n",
        "                inputs = inputs[:11]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.GrammarEnrichedLM(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3uZfInhni5Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultipleChoice Section"
      ],
      "metadata": {
        "id": "vaU0OLRhdgBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedForMultipleChoice(GrammarEnrichedPreTrainedModel, TFMultipleChoiceLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            1, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def dummy_inputs(self):\n",
        "        \"\"\" Dummy inputs to build the network.\n",
        "\n",
        "        Returns:\n",
        "            tf.Tensor with dummy inputs\n",
        "        \"\"\"\n",
        "        return {\"input_ids\": tf.constant(MULTIPLE_CHOICE_DUMMY_INPUTS)}\n",
        "\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs,\n",
        "        grammar_ids,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            input_ids = inputs[0]\n",
        "            attention_mask = inputs[1] if len(inputs) > 1 else attention_mask\n",
        "            token_type_ids = inputs[2] if len(inputs) > 2 else token_type_ids\n",
        "            position_ids = inputs[3] if len(inputs) > 3 else position_ids\n",
        "            head_mask = inputs[4] if len(inputs) > 4 else head_mask\n",
        "            inputs_embeds = inputs[5] if len(inputs) > 5 else inputs_embeds\n",
        "            output_attentions = inputs[6] if len(inputs) > 6 else output_attentions\n",
        "            output_hidden_states = inputs[7] if len(inputs) > 7 else output_hidden_states\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            input_ids = inputs.get(\"input_ids\")\n",
        "            attention_mask = inputs.get(\"attention_mask\", attention_mask)\n",
        "            token_type_ids = inputs.get(\"token_type_ids\", token_type_ids)\n",
        "            position_ids = inputs.get(\"position_ids\", position_ids)\n",
        "            head_mask = inputs.get(\"head_mask\", head_mask)\n",
        "            inputs_embeds = inputs.get(\"inputs_embeds\", inputs_embeds)\n",
        "            output_attentions = inputs.get(\"output_attentions\", output_attentions)\n",
        "            output_hidden_states = inputs.get(\"output_hidden_states\", output_hidden_states)\n",
        "            labels = inputs.get(\"labels\", labels)\n",
        "            assert len(inputs) <= 9, \"Too many inputs.\"\n",
        "        else:\n",
        "            input_ids = inputs\n",
        "\n",
        "        if input_ids is not None:\n",
        "            num_choices = shape_list(input_ids)[1]\n",
        "            seq_length = shape_list(input_ids)[2]\n",
        "        else:\n",
        "            num_choices = shape_list(inputs_embeds)[1]\n",
        "            seq_length = shape_list(inputs_embeds)[2]\n",
        "\n",
        "        flat_input_ids = tf.reshape(input_ids, (-1, seq_length)) if input_ids is not None else None\n",
        "        flat_attention_mask = tf.reshape(attention_mask, (-1, seq_length)) if attention_mask is not None else None\n",
        "        flat_token_type_ids = tf.reshape(token_type_ids, (-1, seq_length)) if token_type_ids is not None else None\n",
        "        flat_position_ids = tf.reshape(position_ids, (-1, seq_length)) if position_ids is not None else None\n",
        "        flat_inputs_embeds = (\n",
        "            tf.reshape(inputs_embeds, (-1, seq_length, shape_list(inputs_embeds)[3]))\n",
        "            if inputs_embeds is not None\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        flat_inputs = [\n",
        "            flat_input_ids,\n",
        "            flat_attention_mask,\n",
        "            flat_token_type_ids,\n",
        "            flat_position_ids,\n",
        "            head_mask,\n",
        "            flat_inputs_embeds,\n",
        "            output_attentions,\n",
        "            output_hidden_states,\n",
        "        ]\n",
        "\n",
        "        outputs = self.bert(flat_inputs, training=training)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        reshaped_logits = tf.reshape(logits, (-1, num_choices))\n",
        "\n",
        "        outputs = (reshaped_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, reshaped_logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), reshaped_logits, (hidden_states), (attentions)\n",
        "\n"
      ],
      "metadata": {
        "id": "nTv3ae4_dgSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Token Classification Section"
      ],
      "metadata": {
        "id": "07q05aP4OcXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForTokenClassification(GrammarEnrichedPreTrainedModel, TFTokenClassificationLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.dropout = tf.keras.layers.Dropout(configuration.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"classifier\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        grammar_embeds=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        labels=None,\n",
        "        training=False,\n",
        "    ):\n",
        "       \n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            labels = inputs[8] if len(inputs) > 8 else labels\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            labels = inputs.pop(\"labels\", labels)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            grammar_embeds=grammar_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        sequence_output = self.dropout(sequence_output, training=training)\n",
        "        logits = self.classifier(sequence_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss = self.compute_loss(labels, logits)\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qEzPy2OpOr6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## QA Section"
      ],
      "metadata": {
        "id": "o82nViPuOVhn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrammarEnrichedLMForQuestionAnswering(GrammarEnrichedPreTrainedModel, TFQuestionAnsweringLoss):\n",
        "    def __init__(self, configuration, *inputs, **kwargs):\n",
        "        super().__init__(configuration, *inputs, **kwargs)\n",
        "        self.num_labels = configuration.num_labels\n",
        "\n",
        "        self.GrammarEnrichedLM = GrammarEnrichedMainLayer(configuration, name=\"GrammarEnrichedLM\")\n",
        "        self.qa_outputs = tf.keras.layers.Dense(\n",
        "            configuration.num_labels, kernel_initializer=get_initializer(configuration.initializer_range), name=\"qa_outputs\"\n",
        "        )\n",
        "\n",
        "    def call(\n",
        "        self,\n",
        "        inputs=None,\n",
        "        grammar_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        grammar_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        start_positions=None,\n",
        "        end_positions=None,\n",
        "        training=False,\n",
        "    ):\n",
        "\n",
        "        if isinstance(inputs, (tuple, list)):\n",
        "            start_positions = inputs[8] if len(inputs) > 8 else start_positions\n",
        "            end_positions = inputs[9] if len(inputs) > 9 else end_positions\n",
        "            if len(inputs) > 8:\n",
        "                inputs = inputs[:8]\n",
        "        elif isinstance(inputs, (dict, BatchEncoding)):\n",
        "            start_positions = inputs.pop(\"start_positions\", start_positions)\n",
        "            end_positions = inputs.pop(\"end_positions\", start_positions)\n",
        "\n",
        "        outputs = self.bert(\n",
        "            inputs,\n",
        "            grammar_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            grammar_embed=grammar_embeds,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            training=training,\n",
        "        )\n",
        "\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
        "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
        "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
        "\n",
        "        outputs = (start_logits, end_logits,) + outputs[2:]\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            labels = {\"start_position\": start_positions}\n",
        "            labels[\"end_position\"] = end_positions\n",
        "            loss = self.compute_loss(labels, outputs[:2])\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), start_logits, end_logits, (hidden_states), (attentions)\n"
      ],
      "metadata": {
        "id": "-tz6UE6pObD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}